{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_04_Operations_on_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-h_wDcNlH_K"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Kjvk_h1AHl"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBca_LsWxnx2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7irzXK6Nvt8b"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkKGBZRvEwZL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 04 - Operations with DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcr9KTJbJI_4"
      },
      "source": [
        "We are going to see different operations that can be performed with DataFrames:\n",
        "\n",
        "  - Row filtering\n",
        "  - Sorting and grouping\n",
        "  - Joins\n",
        "  - Scalar functions and aggregations\n",
        "  - Using them with complex types\n",
        "  - Window functions\n",
        "  - User-defined functions\n",
        "\n",
        "We will end up seeing how to use SQL requests on DataFrames\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH1tMsjx8-sl"
      },
      "source": [
        "As for reading, Spark can save DateFrames in multiple formats:\n",
        "\n",
        "- CSV, JSON, Parquet, Hadoop...\n",
        "\n",
        "It can write them as well on a database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8HjKeILIiCI"
      },
      "source": [
        "#Retrieve a DataFrame reading it from the Parquet format\n",
        "dfSE = spark.read\\\n",
        "            .format(\"parquet\")\\\n",
        "            .option(\"mode\", \"FAILFAST\")\\\n",
        "            .load(os.environ[\"DRIVE_DATA\"] + \"dfSE.parquet\")\n",
        "dfSE.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QynVQ-HVfAmQ"
      },
      "source": [
        "dfSE.show(5)\n",
        "dfSE.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1IQyhIfHnb"
      },
      "source": [
        "## Filter operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hs2OsMefIXm"
      },
      "source": [
        "# Select those posts that contain the word 'Italiano' in their body\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "colBody = col(\"body\")\n",
        "dfItaliano = dfSE.filter(colBody.like('%Italiano%'))\n",
        "\n",
        "print(\"Number of posts with the word Italiano: {0}\\n\".format(dfItaliano.count()))\n",
        "\n",
        "print(\"Show the first line\")\n",
        "dfItaliano.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mlCziy5fPI7"
      },
      "source": [
        "# Retrieve the questions (postType == 1) which have an accepted reply (acceptedAnswerID != null)\n",
        "# Note: where() is an alias of filter()\n",
        "\n",
        "colPostType = col(\"postType\")\n",
        "colAcceptedReplyId = col(\"acceptedAnswerId\")\n",
        "\n",
        "dfQuestionWithAcceptedReply = dfSE\\\n",
        "                    .where((colPostType == 1) & (colAcceptedReplyId.isNotNull()))\\\n",
        "                    .withColumnRenamed(\"Creation_date\", \"Date_of_creation\")\n",
        "\n",
        "print(\"Number of questions with an accepted reply: {0}\"\\\n",
        "      .format(dfQuestionWithAcceptedReply.count()))\n",
        "\n",
        "dfQuestionWithAcceptedReply.cache()\n",
        "\n",
        "dfQuestionWithAcceptedReply\\\n",
        "        .select(\"Date_of_creation\", colPostType.alias(\"Post Type\"), colAcceptedReplyId)\\\n",
        "        .show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQB_ERIcfTGO"
      },
      "source": [
        "# Keep the entries corresponding to June 2014\n",
        "from datetime import date\n",
        "\n",
        "colCreationDate = col(\"Date_of_creation\")\n",
        "\n",
        "dfQuestionWithAcceptedReplyJun14 = dfQuestionWithAcceptedReply\\\n",
        "                    .filter((colCreationDate >= date(2014,6,1)) &\n",
        "                            (colCreationDate <= date(2014,6,30)))\n",
        "\n",
        "dfQuestionWithAcceptedReplyJun14.select(colCreationDate, colPostType, colAcceptedReplyId).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzquPGw-fjHu"
      },
      "source": [
        "# Add a column with the ratio between the number of visits and the score of the question\n",
        "colNumViews = col(\"numViewed\")\n",
        "colPoints = col(\"score\")\n",
        "dfQuestionWithAcceptedReplyRatio = dfQuestionWithAcceptedReply.withColumn(\"ratio\", colNumViews/colPoints)\n",
        "\n",
        "# Shows some columns with ratio > 35\n",
        "colRatio = col(\"ratio\")\n",
        "dfQuestionWithAcceptedReplyRatio.filter(colRatio > 35)\\\n",
        "                        .select(colCreationDate, colNumViews, colPoints, colRatio)\\\n",
        "                        .show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk517hNifmmC"
      },
      "source": [
        "## Sorting and grouping operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l3saJy9fnZD"
      },
      "source": [
        "# Sorting by viewCount\n",
        "dfQuestionWithAcceptedReply.orderBy(colNumViews, ascending=False)\\\n",
        "                  .select(colCreationDate, colNumViews)\\\n",
        "                  .show(10, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B79cu3aqfpmR"
      },
      "source": [
        "# Grouping by the userId column\n",
        "colUserId = col(\"userId\")\n",
        "groupByUser = dfQuestionWithAcceptedReply.groupBy(colUserId)\n",
        "print(type(groupByUser))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQkxaC1dfrvh"
      },
      "source": [
        "print(\"DataFrame with the number of posts by user\")\n",
        "dfPostsByUser = groupByUser.count()\n",
        "dfPostsByUser.printSchema()\n",
        "\n",
        "colNPosts = col(\"count\")\n",
        "dfPostsByUser.select(colUserId.alias(\"User number\"),\n",
        "                        colNPosts.alias(\"Number of posts\"))\\\n",
        "                .orderBy(colUserId).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6_KnTBoft2b"
      },
      "source": [
        "print(\"DataFrame with the average number of views per user\")\n",
        "dfAvgPerUser = groupByUser.avg(\"numViewed\")\n",
        "dfAvgPerUser.orderBy(colUserId).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWD4Hv7rfv0b"
      },
      "source": [
        "# The 'agg' method allows grouping operations expressed as a dictionary {column_name:operation}\n",
        "print(\"Obtain the previous tables with a single operation\")\n",
        "dfCountAvg = groupByUser.agg({\"userId\":\"count\", \"numViewed\":\"avg\"})\n",
        "dfCountAvg.printSchema()\n",
        "\n",
        "colCount = col(\"count(userId)\")\n",
        "colMedia = col(\"avg(numViewed)\")\n",
        "dfCountAvg.select(colUserId.alias(\"User number\"),\n",
        "                   colCount.alias(\"Number of posts\"),\n",
        "                   colMedia.alias(\"Views average\"))\\\n",
        "                  .orderBy(colUserId).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAiW6qzffyN4"
      },
      "source": [
        "# Grouping on two columns\n",
        "dfSE.groupBy(colUserId, colPostType)\\\n",
        "    .count()\\\n",
        "    .sort(colUserId, colPostType)\\\n",
        "    .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVki8IlIf3kU"
      },
      "source": [
        "A description of the functions used with GroupedData can be found on https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#grouping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JHE1mPef7BI"
      },
      "source": [
        "### Advanced grouping\n",
        "\n",
        "It is possible to group data on more than one column: `Rollups` and `Cube`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5N2QB-qf-Dp"
      },
      "source": [
        "#### Rollups\n",
        "\n",
        "Grouping by multiple columns, including aggregations by the first column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTOuTUEhf0w4"
      },
      "source": [
        "# For each user, count the number of questions (postType == 1) and the number of replies (postType == 2)\n",
        "rollupPerUserAndPostType = dfSE.rollup(\"userId\", \"postType\")\n",
        "print(type(rollupPerUserAndPostType))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4k-5enfgBt0"
      },
      "source": [
        "# DataFrame with the number of posts per user and 'Question' post type\n",
        "# Null fields are aggregation fields. For example:\n",
        "# null null = all posts\n",
        "# 4    null = all posts from user with id 4\n",
        "# 4    1    = all posts of type 1 from user with id 4\n",
        "# NOTE: disregard posts with types 4 and 5.\n",
        "dfPostPerUserAndType = rollupPerUserAndPostType.count()\n",
        "dfPostPerUserAndType.printSchema()\n",
        "dfPostPerUserAndType.select(colUserId.alias(\"User number\"),\n",
        "                             colPostType.alias(\"Post type\"),\n",
        "                             colNPosts.alias(\"Number of posts\"))\\\n",
        "                     .orderBy(colUserId,colPostType)\\\n",
        "                     .show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OAwGuDbgE3A"
      },
      "source": [
        "#### Cubes\n",
        "\n",
        "Similar to Rollups, but going through all dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVNxjbsPgHSW"
      },
      "source": [
        "groupByUserAndPostType = dfSE.cube(\"userId\", \"postType\")\n",
        "print(type(groupByUserAndPostType))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOcJXNHegJSz"
      },
      "source": [
        "# # DataFrame with the number of posts per user and 'Question' post type\n",
        "# Null fields are aggregation fields. For example:\n",
        "# null null = all posts\n",
        "# null 1    = all post of type 1\n",
        "# 4    null = all posts from user with id 4\n",
        "# 4    1    = all posts of type 1 from user with id 4\n",
        "# NOTE: disregard posts with types 4 and 5.\n",
        "dfPostPerUserAndType = groupByUserAndPostType.count()\n",
        "dfPostPerUserAndType.printSchema()\n",
        "dfPostPerUserAndType.select(colUserId.alias(\"User number\"),\n",
        "                             colPostType.alias(\"Post type\"),\n",
        "                             colNPosts.alias(\"Number of posts\"))\\\n",
        "                     .orderBy(colUserId,colPostType)\\\n",
        "                     .show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaTiWe8AgMRe"
      },
      "source": [
        "## Joins\n",
        "Spark offers the possibility of performing multiple types of joins (as in SQL)\n",
        "\n",
        "  - inner, outer, left outer, right outer, left semi, left anti, cross"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wotj0yOHgN8s"
      },
      "source": [
        "# We want to join each question that has an accepted reply with the actual reply chosen as the accepted answer\n",
        "# We join the colAcceptedReplyId field from the questions with the id field from the answers\n",
        "dfQuestions = dfQuestionWithAcceptedReply\\\n",
        "                .select(colUserId, colBody, colAcceptedReplyId)\\\n",
        "                .withColumnRenamed(\"userId\", \"User question\")\\\n",
        "                .withColumnRenamed(\"body\", \"Question\")\\\n",
        "                .withColumnRenamed(\"acceptedAnswerId\", \"ID Accepted Reply\")\n",
        "\n",
        "colId = col(\"id\")\n",
        "dfReplies = dfSE\\\n",
        "                .select(colId, colUserId, colBody)\\\n",
        "                .where(colPostType == 2)\\\n",
        "                .withColumnRenamed(\"id\", \"ID Reply\")\\\n",
        "                .withColumnRenamed(\"userId\", \"User reply\")\\\n",
        "                .withColumnRenamed(\"body\", \"Reply\")\n",
        "\n",
        "nQuestions = dfQuestions.count()\n",
        "nReplies = dfReplies.count()\n",
        "print(\"Number of questions with an accepted reply = {0}\".format(nQuestions))\n",
        "print(\"Number of replies = {0}\".format(nReplies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F53DitxIVijS"
      },
      "source": [
        "dfQuestions.show()\n",
        "dfReplies.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX8OeXkugjid"
      },
      "source": [
        "# Join expression\n",
        "joinExpression = dfQuestions[\"ID Accepted Reply\"] == dfReplies[\"ID Reply\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtLpashygmtI"
      },
      "source": [
        "# Inner join\n",
        "# Include only rows for which the joinExpression is true\n",
        "joinType = \"inner\"\n",
        "dfInner = dfQuestions.join(dfReplies, joinExpression, joinType)\n",
        "nRows = dfInner.count()\n",
        "print(\"Number of rows = {0}\".format(nRows))\n",
        "dfInner.show(nRows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V340E9Gagpge"
      },
      "source": [
        "# Outer join\n",
        "# Include all rows from both DataFrames.\n",
        "# In the case there are no matching values on any of the DataFrames, give a null value.\n",
        "joinType = \"outer\"\n",
        "dfOuter = dfQuestions.join(dfReplies, joinExpression, joinType)\n",
        "nRows = dfOuter.count()\n",
        "print(\"Number of rows = {0}\".format(nRows))\n",
        "dfOuter.show(nRows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjz9xalggrcI"
      },
      "source": [
        "# Left Outer join\n",
        "# Include all rows from the left DataFrame (first DataFrame)\n",
        "# If there are no matching values on the right DataFrame, give a null value.\n",
        "joinType = \"left_outer\"\n",
        "dfLOuter = dfQuestions.join(dfReplies, joinExpression, joinType)\n",
        "nRows = dfLOuter.count()\n",
        "print(\"Number of rows = {0}\".format(nRows))\n",
        "dfLOuter.show(nRows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcJe0BegguEk"
      },
      "source": [
        "# Right Outer join\n",
        "# Include all rows from the right DataFrame (second DataFrame)\n",
        "# If there are no matching values on the left DataFrame, give a null value.\n",
        "joinType = \"right_outer\"\n",
        "dfROuter = dfQuestions.join(dfReplies, joinExpression, joinType)\n",
        "nRows = dfROuter.count()\n",
        "print(\"Number of rows = {0}\".format(nRows))\n",
        "dfROuter.show(nRows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsnCeQGggwAB"
      },
      "source": [
        "# Left Semi join\n",
        "# The result includes all values from the first DataFrame that also exist in the second one.\n",
        "joinType = \"left_semi\"\n",
        "dfLSemi = dfReplies.join(dfQuestions, joinExpression, joinType)\n",
        "nRows = dfLSemi.count()\n",
        "print(\"Number of rows = {0}\".format(nRows))\n",
        "dfLSemi.show(nRows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9yzBSTBgyRE"
      },
      "source": [
        "# Left Anti join\n",
        "# The result includes all values from the first DataFrame that DO NOT exist in the second one.\n",
        "joinType = \"left_anti\"\n",
        "dfLAnti = dfReplies.join(dfQuestions, joinExpression, joinType)\n",
        "nRows = dfLAnti.count()\n",
        "print(\"Number of rows = {0}\".format(nRows))\n",
        "dfLAnti.show(nRows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZxEhZ7ig0X1"
      },
      "source": [
        "# Cross join\n",
        "# Cartesian product, joins each row from the first DataFrame with all rows from the second one.\n",
        "# IT IS STRONGLY ADVISED NOT TO USE IT, BECAUSE IT IS EXTREMELY COSTLY\n",
        "dfCross = dfReplies.crossJoin(dfQuestions)\n",
        "nRows = dfCross.count()\n",
        "print(\"Number of rows = {0}\".format(nRows))\n",
        "dfCross.show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_ay9hNAJZzo"
      },
      "source": [
        "## Scalar functions and aggregations\n",
        "\n",
        "Spark has a wide offer of functions to operate with DataFrames:\n",
        "- Mathematical functions: ``abs``, ``log``, ``hypot``, etc.\n",
        "- Operations with strings: ``lenght``, ``concat``, etc.\n",
        "- Operations with dates: ``year``, ``date_add``, etc.\n",
        "- Aggregation operations: ``min``, ``max``, ``count``, ``avg``, ``sum``, ``sumDistinct``, ``stddev``, ``variance``, ``kurtosis``, ``skewness``, ``first``, ``last``, ``window``, etc.\n",
        "\n",
        "A detailed description of those functions can be found on  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjuscCLLNMM5"
      },
      "source": [
        "from pyspark.sql.functions import datediff, col\n",
        "colLastActivity = col(\"lastActivity\")\n",
        "colCreationDate = col(\"Date_of_creation\")\n",
        "\n",
        "# Search for the question with an accepted answer that was active the longest time\n",
        "# (i.e. with the highest difference between the LastActivity -\"lastActivity\"- and Creation_date)\n",
        "\n",
        "mostActive = dfQuestionWithAcceptedReply.withColumn(\"ActiveTime\",datediff(colLastActivity,colCreationDate))\\\n",
        "            .orderBy(\"ActiveTime\", ascending=False)\\\n",
        "            .head()\n",
        "\n",
        "print(\"The question \\n\\n{0}\\n\\nhas been active {1} days\".\\\n",
        "      format(mostActive.body.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\"), mostActive.ActiveTime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLyOnKhmNRBs"
      },
      "source": [
        "from pyspark.sql.functions import window\n",
        "# Obtain the number of posts per week from each user\n",
        "# Group by userId and a date-of-creation window of one week\n",
        "dfQuestionWithAcceptedReply.groupBy(\n",
        "                   colUserId, window(colCreationDate, \"1 week\").alias(\"Week\"))\\\n",
        "                  .count()\\\n",
        "                  .sort(\"count\", ascending=False)\\\n",
        "                  .show(20,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp82ZMrgNTyI"
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Search the average and maximum of the \"points\" (score) of all rows as well as the total number in the DataFrame\n",
        "dfSE.select(F.avg(colPoints), F.max(colPoints), F.count(colPoints)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6utH2VnNWqQ"
      },
      "source": [
        "# Again, but using 'describe'\n",
        "dfSE.select(colPoints).describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVgR2PuNY2p"
      },
      "source": [
        "# Score histogram\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import matplotlib.pyplot as plt\n",
        "#from io import StringIO\n",
        "import io\n",
        "\n",
        "def show(p):\n",
        "    img = io.StringIO()\n",
        "    p.savefig(img, format='svg')\n",
        "    img.seek(0)\n",
        "#    print (\"%html <div style='width:600px'>\" + img.buf() + \"</div>\")\n",
        "\n",
        "# Obtain a histogram with 10 groups\n",
        "x,y = dfSE.select(colPoints).rdd.flatMap(lambda x:x).histogram(20)\n",
        "\n",
        "# Clean the graph\n",
        "plt.gcf().clear()\n",
        "\n",
        "plt.bar(x[:-1], y, width=1.3)\n",
        "plt.xlabel(u'Score')\n",
        "plt.ylabel(u'Number of occurrences')\n",
        "plt.title(u'Histogram')\n",
        "\n",
        "show(plt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErAk3pekNcBU"
      },
      "source": [
        "\n",
        "## Complex types\n",
        "\n",
        "Spark works with three types of complex data: `structs`, `arrays` and `maps`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz4pgmWvNe03"
      },
      "source": [
        "### Structs\n",
        "\n",
        "DataFrames inside DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT5udaEnNiCq"
      },
      "source": [
        "from pyspark.sql.functions import struct\n",
        "# Create a new DF with a column that combines two existing columns\n",
        "colNumViews = col(\"numViewed\")\n",
        "colNReplies = col(\"nAnswers\")\n",
        "dfStruct = dfSE.select(colId, colNumViews, colNReplies, struct(colNumViews, colNReplies)\\\n",
        "               .alias(\"Viewed_Replied\"))\n",
        "dfStruct.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUOdIc7MNnBc"
      },
      "source": [
        "# Obtain a field of the combined column\n",
        "dfStruct.select(col(\"Viewed_Replied\").getField(\"numViewed\")).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTmdbtBFNpN_"
      },
      "source": [
        "\n",
        "### Arrays\n",
        "\n",
        "Arrays let us work with data as if they were a Python array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlSjiUj1Nun6"
      },
      "source": [
        "*Example*\n",
        "\n",
        "Obtain the number of *tags* for each question with an accepted reply and replace the ``&lt;`` and ``&gt;`` by  ''<'' and ''>''\n",
        "\n",
        "  - \"tags\" from each question are saved in a concatenated way, separated by   ''<'' and ''>'', codified as ``&lt;`` and ``&gt;``\n",
        "\n",
        "`&lt;english-comparison&gt;&lt;translation&gt;&lt;phrase-request&gt;`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG2BVeJLNwKI"
      },
      "source": [
        "# First, obtain a DataFrame without null tags\n",
        "dfSE.show(10)\n",
        "dfNotNullTags = dfSE.dropna(\"any\", subset=[\"tags\"])\n",
        "dfNotNullTags.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssk_TYD1Nzhn"
      },
      "source": [
        "# Add a column with all tags splitted\n",
        "from pyspark.sql.functions import split\n",
        "colTags = col(\"tags\")\n",
        "dfTags = dfNotNullTags.withColumn(\"tag_array\", split(colTags, \"&gt;&lt;\"))\n",
        "dfTags.select(colTags, col(\"tag_array\")).show(10, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78q1cSyKN1o4"
      },
      "source": [
        "from pyspark.sql.functions import size\n",
        "# Show the number of tags of each entry\n",
        "colTag_array = col(\"tag_array\")\n",
        "dfTags.select(colTag_array, size(colTag_array)).show(5, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYVIOWIkN4KS"
      },
      "source": [
        "# Show the second tag of each entry\n",
        "dfTags.selectExpr(\"tag_array\", \"tag_array[1]\").show(5, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Anu7EMN6ce"
      },
      "source": [
        "from pyspark.sql.functions import array_contains\n",
        "# Look up whether the word \"usage\" appears in the tags\n",
        "dfTags.withColumn(\"With_usage\", array_contains(colTag_array, \"&lt;usage\"))\\\n",
        "      .select(colTag_array, col(\"With_usage\")).show(5, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijiJfn6kN8FR"
      },
      "source": [
        "from pyspark.sql.functions import explode\n",
        "# Convert each tag in a row\n",
        "dfTagsRows = dfTags.withColumn(\"Tags2\", explode(colTag_array))\n",
        "dfTagsRows.select(colTags, col(\"Tags2\")).show(10, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNayzA52N9kf"
      },
      "source": [
        "# Remove symbols &lt; y &gt;\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "dfTags = dfTagsRows.withColumn(\"Tags_splitted\", regexp_replace(\"Tags2\", \"&[l,g]t;\", \"\"))\\\n",
        "                   .drop(\"Tags2\")\n",
        "dfTags.select(colTags, col(\"Tags_splitted\")).show(10, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8ryfwuPN_vX"
      },
      "source": [
        "# Number of entries with the \"word-choice\" tag\n",
        "print(\"Number of entries with the word-choice tag = {0}\"\n",
        "      .format(dfTags\n",
        "      .filter(col(\"Tags_splitted\") == \"word-choice\")\n",
        "      .count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0__r_uLjOB5P"
      },
      "source": [
        "### Maps\n",
        "\n",
        "They are created from columns that work as key-value pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmWw5prXOD2G"
      },
      "source": [
        "from pyspark.sql.functions import create_map\n",
        "# Create a column with a key-value map\n",
        "# key: id, value: body\n",
        "dfMap = dfSE.select(create_map(col(\"Creation_date\"), col(\"lastActivity\"))\\\n",
        "            .alias(\"Dates\"))\n",
        "dfMap.show(5, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iI6JeDpOGGs"
      },
      "source": [
        "# We can conduct a search using the key\n",
        "dfMap.selectExpr(\"Dates['2013-11-10 19:58:02.1']\").show(5, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqiaEK7DOIh4"
      },
      "source": [
        "## Window functions\n",
        "\n",
        "Similar to aggregation functions, they operate in groups of rows, returning a single value for each row. This allows, among others:\n",
        "\n",
        "  - To obtain moving averages\n",
        "  - To calculate cumulative sums\n",
        "  - To access values higher than the current row value\n",
        "\n",
        "Basically, a window function calculates a value for each input row from a table based on a group of rows, called *frame*.\n",
        "\n",
        "As window functions we can use the aggregation functions previously seen as well as other additional functions (``cume_dist``, ``dense_rank``, ``lag``, ``lead``, ``ntile``, ``percent_rank``, ``rank``, ``row_number``) specified as *Window function* in https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsNLqnIkOL3R"
      },
      "source": [
        "#### Example 1\n",
        "From the ``dfQuestionWithAcceptedReply`` DataFrame, show the score (column \"points\") maximum per user and for each question, the difference between the question score and the user's maximum score.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8JpMFbQOOSb"
      },
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Specify the windows to partition the rows by the userId column\n",
        "window = Window.partitionBy(colUserId)\n",
        "print(type(window))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPT2WYuYOQd0"
      },
      "source": [
        "# Create a column with the maximum score per user\n",
        "colMaxPoints = F.max(colPoints).over(window)\n",
        "print(type(colMaxPoints))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "471ky0FnOUnj"
      },
      "source": [
        "# Obtain a new DataFrame including the maximum score per user and the difference\n",
        "# between this maximum and each question score\n",
        "dfQuestionWithAcceptedReply.select(colUserId, colId.alias(\"Question\"),\n",
        "                          colPoints, colMaxPoints.alias(\"maxPerUser\"))\\\n",
        "                  .withColumn(\"Difference\", colMaxPoints-colPoints)\\\n",
        "                  .orderBy(colUserId, colId)\\\n",
        "                  .show(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzj45bl2OXlS"
      },
      "source": [
        "#### Example 2\n",
        "Show for each user and question from the ``dfQuestionWithAcceptedReply`` DataFrame  the number of days spent between the previous user question until the current one, and from the current one to the following one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UisxHyv4OZib"
      },
      "source": [
        "# Specify the window to partition the rows by the userId column and sort them by creation day\n",
        "window = Window.partitionBy(colUserId).orderBy(colCreationDate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VwWdogcOghH"
      },
      "source": [
        "# Create a column to reference the previous question (in date)\n",
        "PreviousCol = F.lag(colCreationDate, 1).over(window)\n",
        "# Create a column to reference the following question (in date)\n",
        "FollowingCol = F.lead(colCreationDate, 1).over(window)\n",
        "\n",
        "# Show for each user and question the id of the previous and following questions\n",
        "dfQuestionWithAcceptedReply.select(colUserId, colId, colCreationDate.alias(\"Creation Date\"),\n",
        "                          F.datediff(colCreationDate,PreviousCol).alias(\"Days from\"),\n",
        "                          F.datediff(FollowingCol,colCreationDate).alias(\"Days until\"))\\\n",
        "                  .orderBy(colUserId, colId)\\\n",
        "                  .show(30, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn8Wpc4GO4XE"
      },
      "source": [
        "## User-Defined Functions (UDFs)\n",
        "\n",
        "If we need a function that is not implemented, we can create our own function to operate on columns.\n",
        "\n",
        "**Note:**\n",
        "  - UDFs in Python may be quite inefficient, due to the data serialisation in Python\n",
        "  - It is recommended to code them in Scala or Java (and then call them from Python)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUDo_inWO7s_"
      },
      "source": [
        "#### Example\n",
        "\n",
        "User UDFs to obtain the number of *tags* for each question and change the ``&lt;`` and ``&gt;`` by  ''<'' and ''>''\n",
        "\n",
        "  - The \"tags\" from each question are stored concatenated, separated by  ''<'' and ''>'', and coded as ``&lt;`` and ``&gt;``\n",
        "\n",
        "`&lt;english-comparison&gt;&lt;translation&gt;&lt;phrase-request&gt;`\n",
        "\n",
        "To count the number of tags, it is enough to count the number of times ``&lt;`` appears in the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3UMYrG9OjEg"
      },
      "source": [
        "colTags = col(\"tags\")\n",
        "# Obtain a DataFrame without null tags\n",
        "dfNoNullTags = dfSE.na.drop(\"any\", subset=[\"tags\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xII8Fn74PETO"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "\n",
        "# Define a function that returns the number of &lt; in a string\n",
        "def countTags(tags):\n",
        "    return tags.count('&lt;')\n",
        "\n",
        "# Define a function that replaces &lt and &gt by < and >\n",
        "def replaceTags(tags):\n",
        "    return tags.replace('&lt;', '<').replace('&gt;', '>')\n",
        "\n",
        "# Create udfs from these functions\n",
        "udfCountTags = udf(countTags)\n",
        "udfReplaceTags = udf(replaceTags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP2puRELPPyz"
      },
      "source": [
        "dfNoNullTags.select(udfReplaceTags(colTags).alias(\"Tags\"),\\\n",
        "                          udfCountTags(colTags).alias(\"nTags\"))\\\n",
        "                  .show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsxGCSDBI5BZ"
      },
      "source": [
        "**NOTE:** Only Python and Swift are officially supported languages on Colaboratory. If we want to create the UDFs in Scala using Colaboratory, please follow [this instructions](https://medium.com/@shadaj/machine-learning-with-scala-in-google-colaboratory-e6f1661f1c88) to install and configure a Scala kernel. Otherwise, the following two code blocks will not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l36KWJFKPTBB"
      },
      "source": [
        "// Create the previous functions in Scala\n",
        "def countTagsSc(tags:String):Int = tags.split(\"&lt;\").size - 1\n",
        "def replaceTagsSc(tags:String):String = tags.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\")\n",
        "\n",
        "// Register those functions as a Spark SQL function\n",
        "spark.udf.register(\"udfCountTagsSc\", countTagsSc(_:String):Int)\n",
        "spark.udf.register(\"udfReplaceTagsSc\", replaceTagsSc(_:String):String)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs1igO7hSGBp"
      },
      "source": [
        "dfNoNullTags.printSchema()\n",
        "# Call IDFs Scala using an expression\n",
        "dfNoNullTags.selectExpr(\"udfReplaceTagsSc(tags) AS Tags\",\n",
        "                              \"udfCountTagsSc(tags) AS nTags\")\\\n",
        "                  .show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWQYv4ACSJ26"
      },
      "source": [
        "## Using SQL commands\n",
        "\n",
        "SQL commands executed from Spark are converted to operations on DataFrames\n",
        "\n",
        " - It is possible to run remote commands using the JDBC/ODBC server [Thrift](https://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine)\n",
        " - It can also work with stored data in [Apache Hive](https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables)\n",
        "\n",
        "To use SQL commands on a DataFrame , the DataFrame must be registered as a *table* or *view*.\n",
        "\n",
        " - The view can be created as a temporary one (it is deleted when the session ends) or as a global one (kept between sessions).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34_7b-2GSNUy"
      },
      "source": [
        "# Registers the dfQuestionWithAcceptedReply DataFrame as a temporary view\n",
        "dfQuestionWithAcceptedReply.createOrReplaceTempView(\"table_QuestionWithAcceptedReply\")\n",
        "\n",
        "# Create a table with the data stored in Parquet\n",
        "spark.sql(\"CREATE TABLE table_SE USING PARQUET OPTIONS (path '\"+os.environ[\"DRIVE_DATA\"] + \"dfSE.parquet\" + \"')\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuVJ6wzgSPzd"
      },
      "source": [
        "spark.sql(\"SELECT * FROM table_SE\").printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xJd_XlmSRqH"
      },
      "source": [
        "# Run a SQL command on the table contents\n",
        "dfUser100 = spark.sql(\"\"\"SELECT userId,id FROM table_SE\n",
        "                         WHERE userId >= 100\"\"\")\n",
        "dfUser100.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AyFArOXST49"
      },
      "source": [
        "# Show the created tables\n",
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SvrtcygSVjj"
      },
      "source": [
        "# Create a new DataFrame from one of the tables\n",
        "dfFromTable = spark.sql(\"SELECT * FROM table_QuestionWithAcceptedReply\")\n",
        "dfFromTable.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FusaDLDZSXeB"
      },
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS table_QuestionWithAcceptedReply\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS table_SE\")\n",
        "\n",
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkJSOev3XOPn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ppnan-_ELFI"
      },
      "source": [
        "## Exercise 4.1: Pi Estimation\n",
        "\n",
        "Using the Monte Carlo method, estimate the value of Pi. Use the random() method from the random class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkWxWdJ5Vk8"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vPYCnS8ERmg"
      },
      "source": [
        "## Exercise 4.2: Inspect a log file\n",
        "\n",
        "Upload the file /var/log/syslog from your computer to this notebook. Then, select only the \"bad lines\": WARNING and ERROR messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR-Men-G5ZdZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}