{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_03_Working_with_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-h_wDcNlH_K"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 2.4.4, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Kjvk_h1AHl"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-4asPkCgnVB"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pth1GUUrgnUW"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tfoycrngnSJ"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkKGBZRvEwZL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 03 - Working with DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcr9KTJbJI_4"
      },
      "source": [
        "## Introduction to DataFrames\n",
        "We will see:\n",
        "\n",
        "  - How to create a DataFrame\n",
        "  - Basic operations on DataFrames\n",
        "      - Show rows\n",
        "      - Select columns\n",
        "      - Rename, add and delete columns\n",
        "      - Delete null values and duplicated rows\n",
        "      - Replace values\n",
        "  - Save DataFrames in different formats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu6TkZeNd5hz"
      },
      "source": [
        "## Creating DataFrames\n",
        "A DataFrame can be created in different ways:\n",
        "\n",
        "  - From a data sequence\n",
        "  - From Row-type objects\n",
        "  - From an RDD or a DataSet\n",
        "  - Reading data from a file\n",
        "      - Like in Hadoop, Spark supports different filesystems: local, HDFS, Amazon S3\n",
        "          - By and large, it supports any data source that can be read with Hadoop\n",
        "      - Spark can access different types of files: plain text, CSV, JSON, [Parquet](https://parquet.apache.org/), [ORC](https://orc.apache.org/), Sequence, etc\n",
        "        -   It also supports compressed files\n",
        "  - Accessing relational databases or noSQL databases\n",
        "    -   MySQL, Postgres, etc. using JDBC/ODBC\n",
        "    -  Hive, HBase, Cassandra, MongoDB, AWS Redshift, etc.\n",
        "    \n",
        "Some examples on how to create DataFrames below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDJ4UH8wfoO7"
      },
      "source": [
        "### From a sequence or a list of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCnbq2bgf5Rd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col,expr\n",
        "# Creating a DataFrame from a range and adding two columns\n",
        "df = spark.range(1,7,2).toDF(\"n\")\n",
        "df.show()\n",
        "df.withColumn(\"n1\", col(\"n\")+1).withColumn(\"n2\", expr(\"2*n\")).show()\n",
        "# Note that in the call to 'expr' we can include SQL code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxcY86E-f9Ub"
      },
      "outputs": [],
      "source": [
        "# DataFrame from a list of tuples\n",
        "l = [(\"Eric\", 5.1, \"Pass\"),\\\n",
        "     (\"John\", 4.0, \"Fail\"),\\\n",
        "     (\"Manuel\", None, None)]\n",
        "dfMarks = spark.createDataFrame(l, schema=[\"Name\", \"mark\", \"result\"])\n",
        "dfMarks.show()\n",
        "dfMarks.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH7LS_dYgCmH"
      },
      "source": [
        "### Creating DataFrames with a schema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g88zsnXygSlG"
      },
      "source": [
        "When creating a DataFrame, it is a good idea to specify its schema:\n",
        "\n",
        "  - The schema defines the names and data types of each column\n",
        "  - It uses an object of type ``StructType`` to define the name and type of the columns\n",
        "  - The data types used by Spark are defined in:\n",
        "      - For PySpark: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types\n",
        "      - For Scala: https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.types.package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uStMKrWmgcfz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructField, StructType, FloatType, StringType\n",
        "from pyspark.sql import Row\n",
        "# Define the DataFrame schema\n",
        "schemaMarks = StructType([\n",
        "    StructField(\"Name\", StringType(), False),\n",
        "    StructField(\"mark\", FloatType(), True),\n",
        "    StructField(\"result\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "# Create the DataFrame from a list of Row objects\n",
        "rows = [Row(\"Eric\", 5.1, \"Pass\"),\\\n",
        "         Row(\"John\", 4.0, \"Fail\"),\\\n",
        "         Row(\"Manuel\", None, None)]\n",
        "\n",
        "dfMarks = spark.createDataFrame(rows, schema=schemaMarks)\n",
        "dfMarks.show()\n",
        "dfMarks.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-C7Wg96gd5s"
      },
      "source": [
        "### Creating DataFrames from a text file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9bRWavNgk0U"
      },
      "source": [
        "Each file line is stored as a row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nxsEccsgqER"
      },
      "outputs": [],
      "source": [
        "# Mount first the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dfQuijote = spark.read.text(os.environ[\"DRIVE_DATA\"] + \"/quijote.txt\")\n",
        "dfQuijote.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbijvVbcgtw3"
      },
      "source": [
        "### Creating DataFrames from a CSV file (revisited)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDIe3EAEg4xo"
      },
      "source": [
        "As an example, we are going to use a file with questions and replies from Stack Exchange (https://stackexchange.com/) in Italian.\n",
        "It is a CVS file, with the following 13 fields:\n",
        "\n",
        "  0. ``nComs`` - Number of comments of the question of the reply\n",
        "  2. ``lastActivity`` - Date and hour of the last modification\n",
        "  3. ``userId`` - Owner's ID\n",
        "  4. ``body`` - Text of the question or reply\n",
        "  5. ``score`` - Score of the question or reply based on positive and negative votes\n",
        "  6. ``creationDate`` - Creation date and hour\n",
        "  6. ``numViewed`` - Number of times viewed (null if the question has never been viewed)\n",
        "  7. ``title`` - Question title (null if it is a reply)\n",
        "  8. ``tags`` - Tags assigned to the question (null if there are no tags assigned)\n",
        "  9. ``nAnswers`` - Number of replies related to the question (null if there are not any)\n",
        "  10. ``acceptedAnswerId`` - The ID of the accepted answer (null if the question has no accepted answer)\n",
        "  11. ``postType`` - Type of message: 1 question, 2 reply\n",
        "  12. ``id`` - Unique message identifier\n",
        "\n",
        "Fields are separated by the \"~\" symbol"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H07ygUKhL0p"
      },
      "source": [
        "#### a) Read the file and infer the schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2Lzn1GMhXxz"
      },
      "outputs": [],
      "source": [
        "dfSEInferred = spark.read.format(\"csv\")\\\n",
        "                    .option(\"mode\", \"FAILFAST\")\\\n",
        "                    .option(\"sep\", \"~\")\\\n",
        "                    .option(\"inferSchema\", \"true\")\\\n",
        "                    .option(\"header\", \"false\")\\\n",
        "                    .option(\"nullValue\", \"null\")\\\n",
        "                    .option(\"compression\", \"bzip2\")\\\n",
        "                    .load(os.environ[\"DRIVE_DATA\"] +\"italianPosts.csv.bz2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTjGjjNNhYUf"
      },
      "source": [
        "Some options:\n",
        "\n",
        "1. ``mode``: specifies what to do when it finds corrupted entries\n",
        "    - ``PERMISSIVE``: sets all fields to null when a corrupted entry is found (default value)\n",
        "    - ``DROPMALFORMED``: deletes the rows with corrupted entries\n",
        "    - ``FAILFAST``: returns an error when a corrupted entry is found\n",
        "2. ``sep``:  field delimiter (by default \",\")\n",
        "3. ``inferSchema``: whether column types must be inferred (by default \"false\")\n",
        "4. ``header``: if \"true\", the first line is taken as the header (by default \"false\")\n",
        "5. ``nullValue``: character or string thar represents a NULL in the file  (by default \"\")\n",
        "6. ``compression``: compression type (by default \"none\")\n",
        "  \n",
        "These options are similar for other types of files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1atd4MqhghB"
      },
      "outputs": [],
      "source": [
        "# Show 5 rows\n",
        "dfSEInferred.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjM7KCurhkRv"
      },
      "outputs": [],
      "source": [
        "# Find out how the schema was inferred\n",
        "dfSEInferred.schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4Q9AQ4dhn5L"
      },
      "outputs": [],
      "source": [
        "# Another way of getting the same result\n",
        "dfSEInferred.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbK_qvf3hs5v"
      },
      "source": [
        "#### b) Read the file and specify the schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1xnLchfhzD6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "# We first create a list with each column header\n",
        "# Note: avoid spaces and non-ascii characters on column names\n",
        "header = ([\"nComs\", \"lastActivity\", \"userId\",\n",
        "            \"body\", \"score\", \"creationDate\", \"numViewed\", \"title\",\n",
        "            \"tags\", \"nAnswers\", \"acceptedAnswerId\", \"postType\", \"id\"])\n",
        "\n",
        "# Define the schema for the elements of the table\n",
        "# StructType -> Defines a schema for the DF from a list of StructFields\n",
        "# StructField -> Defines the name and type of each column, and whether it is nullable or not (True field)\n",
        "dfSE_Schema = StructType([\n",
        "  StructField(header[0], IntegerType(), True),\n",
        "  StructField(header[1], TimestampType(), True),\n",
        "  StructField(header[2], LongType(), True),\n",
        "  StructField(header[3], StringType(), True),\n",
        "  StructField(header[4], IntegerType(), True),\n",
        "  StructField(header[5], TimestampType(), True),\n",
        "  StructField(header[6], IntegerType(), True),\n",
        "  StructField(header[7], StringType(), True),\n",
        "  StructField(header[8], StringType(), True),\n",
        "  StructField(header[9], IntegerType(), True),\n",
        "  StructField(header[10], LongType(), True),\n",
        "  StructField(header[11], ByteType(), True),\n",
        "  StructField(header[12], LongType(), True)\n",
        "  ])\n",
        "\n",
        "dfSE = spark.read.format(\"csv\")\\\n",
        "                    .option(\"mode\", \"FAILFAST\")\\\n",
        "                    .option(\"sep\", \"~\")\\\n",
        "                    .option(\"inferSchema\", \"false\")\\\n",
        "                    .option(\"header\", \"false\")\\\n",
        "                    .option(\"nullValue\", \"null\")\\\n",
        "                    .option(\"compression\", \"bzip2\")\\\n",
        "                    .schema(dfSE_Schema)\\\n",
        "                    .load(os.environ[\"DRIVE_DATA\"] +\"italianPosts.csv.bz2\")\n",
        "dfSE.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwS3ZIcFh3y2"
      },
      "outputs": [],
      "source": [
        "dfSE.sort(\"id\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1wlNqCsh4Wv"
      },
      "outputs": [],
      "source": [
        "dfSE.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7JqL8PojOEa"
      },
      "source": [
        "## Basic operations with DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx69XYmUjUP7"
      },
      "source": [
        "### Show rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRtkoWUZjaHO"
      },
      "outputs": [],
      "source": [
        "# show(n) shows the first n rows (by default, n=20)\n",
        "dfSE.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4Nyk4RrjePd"
      },
      "outputs": [],
      "source": [
        "# Say that we do not want to truncate the long fields\n",
        "dfSE.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1BBaaz-jgp-"
      },
      "outputs": [],
      "source": [
        "# take(n) returns the first n rows as a Python list of Row objects\n",
        "list = dfSE.take(5)\n",
        "print(list[1])\n",
        "print(\"\\n\")\n",
        "# collect() returns the DataFrame as a Python list of Row objects\n",
        "# Warning: if the DataFrame is too large, it might collapse the Driver!\n",
        "list2 = dfSE.collect()\n",
        "print(list2[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPDx7xvIjif4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# sample(withReplacement, fraction, seed=None) returns a new Dataframe with a fraction of the original rows\n",
        "dfSESampled = dfSE.sample(False, 0.1, seed=None)\n",
        "print(\"Original Number of rows = {0}; Number of sampled rows = {1}\".format(dfSE.count(), dfSESampled.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-DOlABOjkc0"
      },
      "outputs": [],
      "source": [
        "# limit(n) limits the number of rows calculated to n\n",
        "dfSE_10rows = dfSE.sample(False, 0.1, seed=None).limit(10)\n",
        "print(\"Number of sampled rows = {0}\".format(dfSE_10rows.count()))\n",
        "dfSE_10rows.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HXAspRdjnV-"
      },
      "source": [
        "### Execute an operation on each row\n",
        "The method `foreach` applies a function to each row\n",
        "\n",
        "- The DataFrame is not modified and no other DataFrames are created\n",
        "- `foreach` is executed in the Workers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3QC31oFjvjx"
      },
      "outputs": [],
      "source": [
        "def printid(f):\n",
        "    print(f[\"id\"])\n",
        "\n",
        "# In theory, this code should print all values of the 'id' column.\n",
        "# Due to the way the notebook manages tasks, it is not possible to see any output.\n",
        "# Run it on a pyspark-shell to see the output.\n",
        "dfSE_10rows.foreach(printid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTZ2CtybjwQn"
      },
      "source": [
        "### Select columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQamd1S0jznG"
      },
      "outputs": [],
      "source": [
        "# Creates a new DataFrame by selecting columns by name\n",
        "dfIdBody = dfSE.select(\"id\", \"body\")\n",
        "dfIdBody.show(5)\n",
        "\n",
        "print(\"The idBody object is of type {0}\".format(type(dfIdBody)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYlZy8j4j2sR"
      },
      "outputs": [],
      "source": [
        "# Another way of specifying the columns to select\n",
        "dfIdBody2 = dfSE.select(dfSE.id, dfSE.body)\n",
        "dfIdBody2.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQWu1PaXj3aD"
      },
      "outputs": [],
      "source": [
        "# It is also possible to specify objects of Column type...\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "colId = col(\"id\")\n",
        "colCreateDate = col(\"creationDate\")\n",
        "print(\"The colId object is of type {0}\".format(type(colId)))\n",
        "print(\"The colCreateDate object is of type {0}\".format(type(colCreateDate)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SvgG2Sjj5BW"
      },
      "outputs": [],
      "source": [
        "# ... and create a DataFrame from Column objects, by renaming the columns\n",
        "dfIdBodyDate = dfSE.select(colId,\n",
        "                              colCreateDate.alias(\"Creation_date\"),\n",
        "                              dfSE.body.alias(\"Content\"))\n",
        "dfIdBodyDate.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw45U9EYj9Nh"
      },
      "source": [
        "#### Select columns by using expressions\n",
        "\n",
        "To select columns using SQL expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSTG7pXokAd5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "# Same DataFrame as before but using expressions\n",
        "dfIdDateBodyExpr = dfSE.select(\n",
        "                           expr(\"id AS ID\"),\n",
        "                           expr('creationDate AS Creation_date'),\n",
        "                           expr(\"body AS Content\"))\n",
        "dfIdDateBodyExpr.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO43v_chkCg-"
      },
      "outputs": [],
      "source": [
        "# We can use more complex expressions\n",
        "dfSE.selectExpr(\"*\", # Select all columns and set ValidReply to True for those with, at least, one reply.\n",
        "                \"(nAnswers IS NOT NULL) as ValidReply\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3fIFG0MkHY0"
      },
      "source": [
        "### Rename, add and delete columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4-CC8StkJ1r"
      },
      "outputs": [],
      "source": [
        "# Rename the creationDate column\n",
        "dfSE = dfSE.withColumnRenamed(\"creationDate\", \"Creation_date\")\n",
        "dfSE.cache()\n",
        "dfSE.select(\"Creation_date\",\n",
        "            dfSE.numViewed.alias(\"Number_of_visits\"),\n",
        "            \"score\",\n",
        "            \"postType\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFWWZ4askPrV"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'ones' with all its values set to 1\n",
        "from pyspark.sql.functions import lit\n",
        "# lit transforms a literal in Python to Spark internal format\n",
        "# (in this example, IntegerType)\n",
        "dfSE = dfSE.withColumn(\"ones\", lit(1))\n",
        "dfSE.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wWzrupfkRth"
      },
      "outputs": [],
      "source": [
        "# Removes a column using drop\n",
        "dfSE = dfSE.drop(col(\"ones\"))\n",
        "dfSE.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsxTogrdkWt0"
      },
      "source": [
        "### Delete null and duplicated values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XT8uw47kY2k"
      },
      "outputs": [],
      "source": [
        "# Remove all rows that have null on any of their columns\n",
        "dfNoNulls = dfSE.dropna(\"any\")\n",
        "print(\"Initial number or rows: {0}; number of non null rows: {1}\"\n",
        "       .format(dfSE.count(), dfNoNulls.count()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0xh2BW5ka_A"
      },
      "outputs": [],
      "source": [
        "# Remove rows that have null on all their columns\n",
        "dfNeitherNull = dfSE.dropna(\"all\")\n",
        "print(\"Number of rows with all columns set to null: {0}\"\n",
        "       .format(dfSE.count() - dfNeitherNull.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYKhJBTQkdbR"
      },
      "outputs": [],
      "source": [
        "# Remove duplicated rows\n",
        "dfWithoutDuplicates = dfSE.dropDuplicates()\n",
        "print(\"Number of duplicated rows: {0}\"\n",
        "       .format(dfSE.count() - dfWithoutDuplicates.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Fgz5rXhkf-C"
      },
      "outputs": [],
      "source": [
        "# Remove rows when a given column is duplicated\n",
        "dfWithoutDuplicatedUser = dfSE.dropDuplicates([\"userId\"])\n",
        "print(\"Number of unique users: {0}\"\n",
        "       .format(dfWithoutDuplicatedUser.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8CSyfr1kjWe"
      },
      "outputs": [],
      "source": [
        "# Other examples\n",
        "dfNoNullnumViewedAcceptedAnswerId = dfSE.dropna(\"any\", subset=[\"numViewed\", \"acceptedAnswerId\"])\n",
        "print(\"Number of rows with numViewed AND acceptedAnswerId not null: {0}\"\n",
        "       .format(dfNoNullnumViewedAcceptedAnswerId.count()))\n",
        "\n",
        "dfNoNullnumViewedAcceptedAnswerId = dfSE.dropna(\"all\", subset=[\"numViewed\", \"acceptedAnswerId\"])\n",
        "print(\"Number of rows with numViewed OR acceptedAnswerId not null: {0}\"\n",
        "       .format(dfNoNullnumViewedAcceptedAnswerId.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR8rAZGCkln-"
      },
      "source": [
        "### Replacing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGtTf0FmkuXv"
      },
      "outputs": [],
      "source": [
        "# Replace with '0' all null values in the numVistas and nAnswers fields\n",
        "dfSE = dfSE.fillna(0, subset=[\"numViewed\", \"nAnswers\"])\n",
        "dfSE.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILZkZtlAkwd4"
      },
      "outputs": [],
      "source": [
        "# Replace the value 1170 with 3000 in columns \"id\" and \"acceptedAnswerId\"\n",
        "dfSE.select(\"id\", \"acceptedAnswerId\").show(10)\n",
        "dfSE.replace(1170, 3000, subset=[\"id\", \"acceptedAnswerId\"])\\\n",
        "    .select(\"id\", \"acceptedAnswerId\")\\\n",
        "    .show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0AhRhpzky7X"
      },
      "source": [
        "## Saving DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVR4GjW2k2S6"
      },
      "source": [
        "As for reading, Spark can save DateFrames in multiple formats:\n",
        "\n",
        "- CSV, JSON, Parquet, Hadoop...\n",
        "\n",
        "It can write them as well on a database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY8vG9xbk78E"
      },
      "outputs": [],
      "source": [
        "# Save the dfSE DataFrame in JSON format\n",
        "#dfSE.write.format(\"json\").mode(\"overwrite\").save(\"/content/dfSE.json\")\n",
        "dfSE.write.json(os.environ[\"DRIVE_DATA\"] + \"dfSE.json\",mode=\"overwrite\")\n",
        "\n",
        "#!mv /content/dfSE.json \"$DRIVE_DATA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZdDPlv1lGzn"
      },
      "outputs": [],
      "source": [
        "!ls -alh \"$DRIVE_DATA\"/dfSE.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9Q-63hElJYQ"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame using Parquet\n",
        "dfSE.write.format(\"parquet\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .save(os.environ[\"DRIVE_DATA\"] + \"dfSE.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp6xMAnLlMJU"
      },
      "outputs": [],
      "source": [
        "# Parquet uses by default the Snappy compressed format\n",
        "!ls -alh \"$DRIVE_DATA\"/dfSE.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPpyVB30lPd-"
      },
      "source": [
        "It will create as many files as there are partitions in the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUTF4Q8QlRhI"
      },
      "outputs": [],
      "source": [
        "dfSE2 = dfSE.repartition(2)\n",
        "# Save the DataFrame using Parquet, with gzip compression\n",
        "dfSE2.write.format(\"parquet\")\\\n",
        "     .mode(\"overwrite\")\\\n",
        "     .option(\"compression\", \"gzip\")\\\n",
        "     .save(os.environ[\"DRIVE_DATA\"] + \"/dfSE2.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuypMiRilTaZ"
      },
      "outputs": [],
      "source": [
        "!ls -alh \"$DRIVE_DATA\"/dfSE2.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuQmqkp1lbU9"
      },
      "source": [
        "### Partitioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2YPJVuWlfTB"
      },
      "source": [
        "Spark can partition and save a file using the value of a given column\n",
        "\n",
        "- A directory is created for each different value in the partitioning column\n",
        "    - All data associated to that value are stored in that directory\n",
        "- It simplifies the access to the values associated to a given key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMXPmB-6lii6"
      },
      "outputs": [],
      "source": [
        "# Save our DataFrame partitioned by the userID field (using Parquet)\n",
        "dfSE.write.format(\"parquet\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .partitionBy(\"userId\")\\\n",
        "    .save(os.environ[\"DRIVE_DATA\"] + \"dfSE-partition.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HOlEv6olkyI"
      },
      "outputs": [],
      "source": [
        "#!ls -lh \"$DRIVE_DATA\"dfSE-partition.parquet\n",
        "!ls -lh \"$DRIVE_DATA\"dfSE-partition.parquet/userId=10\n",
        "#rm -rf \"$DRIVE_DATA\"dfSE-partition.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkJSOev3XOPn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ULPx4Y1LiR"
      },
      "source": [
        "## Exercise 3.1: Word count\n",
        "\n",
        "Count the number of words *per line* in the $DRIVE_DATA/quijote.txt file.\n",
        "\n",
        "Repeat the exercise but this time counting the number of words *in the whole file*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c7Q_ljrX5RtE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "# so that we can use the F.split() function.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}