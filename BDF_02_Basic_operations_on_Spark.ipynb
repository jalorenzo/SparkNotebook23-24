{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_02_Basic_operations_on_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaD3xvwg2Nnl"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hifcN82z1o_a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Kjvk_h1AHl"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T01OUpE1o7H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AFTUVTO1o4S"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1DWUdD31o0y"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR2QZfJZ1ovu"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHHUo1a7Qp9m"
      },
      "outputs": [],
      "source": [
        "!ls \"$DRIVE_DATA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkKGBZRvEwZL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 02 - Basic operations on Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA37mxxrFCtL"
      },
      "source": [
        "- Spark operates with immutable and distributed collections of elements, managing them in parallel\n",
        "    - Structured API: DataFrames and DataSets\n",
        "    - Low-level API: RDDs\n",
        "\n",
        "-   Operations on these collections\n",
        "    -   Creation\n",
        "    -   Transformations (sorting, filtering, etc.)\n",
        "    -   Actions to obtain results\n",
        "\n",
        "-   Spark automatically distributes data and parallelises operations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxVbhE4NJYF3"
      },
      "source": [
        "## Example: creation of a DataFrame from a CSV file\n",
        "\n",
        "**Note:** To learn how to upload a file into collaboratory from your machine or from Google Drive, check [this link](https://colab.research.google.com/notebooks/io.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS26HN93HsDW"
      },
      "source": [
        "### Option 1: Uploading the *2015-summary.csv* CSV file from your computer\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7opOoMHHvCN"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv553N0FLseK"
      },
      "outputs": [],
      "source": [
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  df = pd.read_csv(io.StringIO(uploaded[fn].decode('utf-8')))\n",
        "  print(format(df.head()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz70AWIsKRGF"
      },
      "outputs": [],
      "source": [
        "!ls -lh 2015-summary.csv\n",
        "!head 2015-summary.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFrEJdMuDT4s"
      },
      "source": [
        "### Option 2: Uploading the CSV file from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1BI6RTZ-BA0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1ozxIns-FG5"
      },
      "outputs": [],
      "source": [
        "!head \"$DRIVE_DATA/2015-summary.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZEO9sioDdQH"
      },
      "source": [
        "### Creating the DataFrame\n",
        "\n",
        "In this example, Spark infers the data schema automatically\n",
        "\n",
        "  - It is better to specify the schema in a explicit way, as we will see later\n",
        "\n",
        "We define the first line to be the header."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO79mIbNMUFS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "flightData2015 = (spark\n",
        "    .read\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .csv(os.environ[\"DRIVE_DATA\"] +\"/2015-summary.csv\"))\n",
        "\n",
        "flightData2015.printSchema()\n",
        "\n",
        "flightData2015.show(5)\n",
        "print(flightData2015.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KYSQqBGE2Qr"
      },
      "source": [
        "## Rows\n",
        "\n",
        "Rows in a DataFrame are objects of `Row`  type\n",
        "\n",
        "- Row API in Python: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html#pyspark.sql.Row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoHDkbcRFlqX"
      },
      "source": [
        "### Row manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbIn1BPpFw1h"
      },
      "outputs": [],
      "source": [
        "# Get the two first rows of the DataFrame\n",
        "row1_2 = flightData2015.take(2)\n",
        "print(row1_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX09DSHkIPGj"
      },
      "outputs": [],
      "source": [
        "# Get the first row as a Python dictionary\n",
        "print(row1_2[0].asDict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJaL8kRM0JSs"
      },
      "source": [
        "## Partitions\n",
        "\n",
        "The elements in a DataFrame (or DataSet or RDD) are splitted between the nodes of the cluster, dividing the collection in partitions. Each partition is then processed by a given executor.\n",
        "\n",
        "-  The number of partitions by default is a function of the cluster size (total number of cores from every executor) and the data size (number of blocks of the files in HDFS)\n",
        "-  In the case of an RDD, a different partition size can be specified at creation time.\n",
        "- The partition size can be modified once they are created.\n",
        "\n",
        "![Partitioning](https://docs.google.com/drawings/d/1GAasfY7P7uaMXhvGHuZ1nOqPqv6TrE7-N96RqUn1NqE/pub?w=960&h=540)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS4EUE5k0RI4"
      },
      "outputs": [],
      "source": [
        "print(\"Number of partitions: {0}\"\n",
        "    .format(flightData2015.rdd.getNumPartitions()))\n",
        "\n",
        "# Create a new DataFrame with 4 partitions\n",
        "flightData2015_4P = flightData2015.repartition(4)\n",
        "print(\"Number of partitions: {0}\"\n",
        "    .format(flightData2015_4P.rdd.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MEZZbQGafhU"
      },
      "source": [
        "##Transformations vs Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riLRJq9_0VFd"
      },
      "source": [
        "### Transformations\n",
        "\n",
        "Operations that transform data\n",
        "\n",
        "  - Origin data are not transformed ( *immutability* )\n",
        "  - Transformations are computed in a \"lazy\" way ( *lazyness* ),  in the sense that they do not actually do anything until an action is executed.\n",
        "\n",
        "Two types:\n",
        "\n",
        "  - *Narrow* Transformations\n",
        "    - Each input partition contributes to a single output partition\n",
        "    - The number of partitions is not modified\n",
        "    - Typically performed in memory\n",
        "  - *Wide* Transformations\n",
        "    - Each output partition depends on several (or all) input partitions\n",
        "    - They imply data shuffling\n",
        "    - The number of partitions can be modified\n",
        "    - They may imply disk writes\n",
        "    \n",
        "Examples:\n",
        "* map\n",
        "* filter\n",
        "* replace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11cYhpn90XM_"
      },
      "outputs": [],
      "source": [
        "# Narrow transformation example\n",
        "flightData2015_EEUU = flightData2015.replace(\"United States\", \"Estados Unidos\")\n",
        "flightData2015_EEUU.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKJkWfYN0Z-u"
      },
      "outputs": [],
      "source": [
        "# Wide transformation example\n",
        "flightData2015_Ord = flightData2015_EEUU.sort(\"count\", ascending=False)\n",
        "flightData2015_Ord.cache()\n",
        "flightData2015_Ord.show(5)  #we don't want to force an action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MENQmQq0faM"
      },
      "source": [
        "### Actions\n",
        "\n",
        "They return a result to the driver program, forcing therefore to perform the pending transformations\n",
        "\n",
        "  - When an action is triggered, a *plan* is created with the transformations needed to obtain the requested data\n",
        "    - A Directed Acyclic Graph (DAG) is created to connect the transformations to apply\n",
        "    - Spark will optimise this graph, removing unnecessary tranformations and joining them when possible\n",
        "  - Actions translate the DAG into an execution plan\n",
        "\n",
        "Types of actions:\n",
        "\n",
        "  - Actions to show data in the console\n",
        "  - Actions to convert Spark data into language-related data\n",
        "  - Actions to write data to disk\n",
        "  \n",
        "Examples:\n",
        "* reduce\n",
        "* collect\n",
        "* take\n",
        "* show\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFCKol2v0gjC"
      },
      "outputs": [],
      "source": [
        "# Action example\n",
        "print(\"Number of rows in the table: {0}\".format(flightData2015_Ord.count()))\n",
        "\n",
        "print(flightData2015_Ord.take(3))\n",
        "\n",
        "flightData2015_Ord.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTmzHsWvanUk"
      },
      "source": [
        "### DAG example\n",
        "Each job is represented by a graph (specifically a directed acyclic graph (DAG)):\n",
        "\n",
        "![DAG](http://2.bp.blogspot.com/-5sDP78mSdlw/Ur3szYz1HpI/AAAAAAAABCo/Aak2Xn7TmnI/s1600/p2.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}