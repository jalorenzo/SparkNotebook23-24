{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_08_Running_Spark_on_a_cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 08 - Running Spark on a cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q4ebRam_m3Z"
      },
      "source": [
        "## Running a Spark program\n",
        "\n",
        "### Command `spark-submit`\n",
        "\n",
        "-   Submits a Spark program (an application) to a cluster\n",
        "-   More specifically, it launches the driver program and invokes the main() method specified by the user\n",
        "\n",
        "-   Examples:\n",
        "```sh\n",
        "$ bin/spark-submit --master yarn --deploy-mode cluster \\  \n",
        "     --py-files anotherlib.zip,anotherfile.py \\  \n",
        "     --num-executors 10 --executor-cores 2 \\  \n",
        "     my-script.py script_options\n",
        "```\n",
        "\n",
        "### `spark-submit` options\n",
        "\n",
        "-   `master`: cluster manager to use (options: `yarn`, `mesos://host:port`, `spark://host:port`, `local[n]`)\n",
        "\n",
        "-   `deploy-mode`: Two ways of deploying\n",
        "\n",
        "    -   `client`: runs the driver on the local node\n",
        "\n",
        "    -   `cluster`: runs the driver on a node of the cluster\n",
        "\n",
        "-   `class`: class to execute (Java or Scala)\n",
        "\n",
        "-   `name`: name of the application (it will be shown in the Spark web)\n",
        "\n",
        "-   `jars`: jar files to add to the classpath (Java o Scala)\n",
        "\n",
        "-   `py-files`: files to add to the PYTHONPATH (`.py`,`.zip`,`.egg`)\n",
        "\n",
        "-   `files`: data files for the applications\n",
        "\n",
        "-   `executor-memory`: total memory of each executor\n",
        "\n",
        "-   `driver-memory`: memory of the driver process\n",
        "\n",
        "For more options: `spark-submit --help`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr0Lfj_3_3Xy"
      },
      "source": [
        "!spark/bin/spark-submit --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MBC4c6_AgWb"
      },
      "source": [
        "![Spark-YARN: Client mode](https://i.pinimg.com/originals/e5/e6/a6/e5e6a6dbc4da4a2dbc1b54effd5995ee.jpg)\n",
        "\n",
        "\n",
        "![Spark-on-YARN: Cluster mode](https://i.pinimg.com/originals/db/16/e9/db16e98baed2a9b54c64e931e1f9b2b5.jpg)\n",
        "\n",
        "Source: [Spark-on-YARN: Empower Spark Applications on Hadoop Cluster](https://www.slideshare.net/Hadoop_Summit/sparkonyarn-empower-spark-applications-on-hadoop-cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0CiVeetECWE"
      },
      "source": [
        "## Configuration parameters\n",
        "\n",
        "Several parameters that can be adjusted in runtime\n",
        "\n",
        "-   In the script\n",
        "```python\n",
        "from pyspark import SparkConf,SparkContext\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.app.name\", \"My app\")\n",
        "conf.set(\"spark.master\", \"local[2]\") # Cluster manager local mode with 2 threads\n",
        "conf.set(\"spark.ui.port\", \"3600\")    # Port of the Spark web interface (by default: 4040)\n",
        "sc = SparkContext(conf=conf)\n",
        "```\n",
        "\n",
        "-   Using flags in the `spark-submit` command\n",
        "```sh\n",
        "$ bin/spark-submit --master local[2] --name \"My app\" \\  \n",
        "    --conf spark.ui.port=3600 my-script.py\n",
        "```    \n",
        "    \n",
        "-   Using a properties file\n",
        "```sh\n",
        "$ cat config.conf\n",
        "spark.master     local[2]\n",
        "spark.app.name   \"My app\"\n",
        "spark.ui.port 3600\n",
        "$ bin/spark-submit --properties-file config.conf my-script.py\n",
        "```\n",
        "\n",
        "More information: <http://spark.apache.org/docs/latest/configuration.html#spark-properties>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKwBitASEZrX"
      },
      "source": [
        "## Example: Python script execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK5ru0Z9JpJO"
      },
      "source": [
        "!cat \"$DRIVE_DATA\"/myscript.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFuf5kOLCrnB"
      },
      "source": [
        "# NOTE: It won't work in a notebook.\n",
        "# Do NOT modify the following line\n",
        "cat << EOF > /tmp/myscript.py\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from operator import add\n",
        "\n",
        "def main():\n",
        "    conf = SparkConf()\n",
        "    conf.set(\"spark.app.name\", \"My Python script\")\n",
        "\n",
        "    # Initialise the SparkContext\n",
        "    sc = SparkContext(conf=conf)\n",
        "    sc.setLogLevel(\"FATAL\")\n",
        "\n",
        "    rdd = sc.parallelize(range(100000)).cache()\n",
        "\n",
        "    rdd2 = rdd.map(lambda x: (x, 2*x))\\\n",
        "              .map(lambda (x,y): (x-100, y**2))\\\n",
        "              .reduceByKey(lambda x,y: x+y)\\\n",
        "              .values()\n",
        "\n",
        "    r = rdd2.reduce(add)\n",
        "\n",
        "    print(\"Final result = {0}\".format(r))\n",
        "\n",
        "    # Stop the SparkContext\n",
        "    sc.stop()\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "EOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLVf_t_RMnM6"
      },
      "source": [
        "!spark/bin/spark-submit --master local[8] \"$DRIVE_DATA\"myscript.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}