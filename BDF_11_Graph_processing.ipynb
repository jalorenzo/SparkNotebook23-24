{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_11_Graph_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.2.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  https://archive.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W8ScIqhafBT"
      },
      "source": [
        "!wget https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop2.7 /content/spark\n",
        "\n",
        "!mv graphframes-0.8.2-spark3.2-s_2.12.jar /content/spark/jars/\n",
        "\n",
        "!export SPARK_HOME=/content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\"\n",
        "\n",
        "!ls -l /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "sc.addPyFile('/content/spark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar')\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"packages\",\"graphframes:graphframes-0.8.2-spark3.2-s_2.12\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tF2L0fcQJMRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 11 - Graph processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc96GG2FFzA5"
      },
      "source": [
        "## GraphX: Graph processing with RDDs\n",
        "\n",
        "Parallel graph programming using Spark\n",
        "\n",
        "- Main abstraction: [*Graph*](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.Graph)\n",
        "    -   Directed multigraph with properties assigned to vertices and edges\n",
        "    -   It is an extension of the RDDs\n",
        "- It includes graph constructors, basic operators ( *reverse*, *subgraph*…) and graph algorithms ( *PageRank*, *Triangle Counting*…)\n",
        "- Only availabe on Scala.\n",
        "\n",
        "Documentation: [spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)\n",
        "\n",
        "API: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szq16u_cIKxA"
      },
      "source": [
        "## Graphs in GraphX\n",
        "<img src=\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/grapxgraph.png\" alt=\"Grafo en GraphX\" style=\"width: 50px;\"/>\n",
        "(Source: M.S. Malak, R. East \"Spark GraphX in action\", Manning, 2016)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HkF-qEfIZrO"
      },
      "source": [
        "### Example of a simple graph\n",
        "<img src=\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/simpsonsgraph.png\" alt=\"Grafo de los Simpson\" style=\"width: 600px;\"/>\n",
        "(Source: P. Zecević, M. Bonaći \"Spark in action\", Manning, 2017)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6bmE-QvGf2r"
      },
      "source": [
        "## GraphFrames: : Graph processing with DataFrames\n",
        "\n",
        "In Python we can use [*GraphFrames*](https://graphframes.github.io/graphframes/docs/_site/quick-start.html) which wraps GraphX algorithms under the DataFrames API, providing a Python interface.\n",
        "\n",
        "- Support for multiple languages is on the works\n",
        "    - For now,  available for Scala and Python\n",
        "- Not yet integrated on Spark\n",
        "    - Available as an external package (https://spark-packages.org/package/graphframes/graphframes)\n",
        "\n",
        "More information:\n",
        "- Project web: https://graphframes.github.io/graphframes/docs/_site/\n",
        "- Python API : https://graphframes.github.io/graphframes/docs/_site/api/python/index.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l07_p2RRIu_f"
      },
      "source": [
        "### Graphs using pyspark and GraphFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1enKNqmMIy1Z"
      },
      "source": [
        "# The following example shows how to create a GraphFrame, query it, and run the PageRank algorithm.\n",
        "# Source: https://graphframes.github.io/graphframes/docs/_site/quick-start.html\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "from graphframes import *\n",
        "\n",
        "# Create a Vertex DataFrame with unique ID column \"id\"\n",
        "v = sqlContext.createDataFrame([\n",
        "  (\"a\", \"Alice\", 34),\n",
        "  (\"b\", \"Bob\", 36),\n",
        "  (\"c\", \"Charlie\", 30),\n",
        "], [\"id\", \"name\", \"age\"])\n",
        "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
        "e = sqlContext.createDataFrame([\n",
        "  (\"a\", \"b\", \"friend\"),\n",
        "  (\"b\", \"c\", \"follow\"),\n",
        "  (\"c\", \"b\", \"follow\"),\n",
        "], [\"src\", \"dst\", \"relationship\"])\n",
        "# Create a GraphFrame\n",
        "\n",
        "g = GraphFrame(v, e)\n",
        "\n",
        "# Query: Get in-degree of each vertex.\n",
        "g.inDegrees.show()\n",
        "\n",
        "# Query: Count the number of \"follow\" connections in the graph.\n",
        "g.edges.filter(\"relationship = 'follow'\").count()\n",
        "\n",
        "# Run PageRank algorithm, and show results.\n",
        "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
        "results.vertices.select(\"id\", \"pagerank\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIZQECRHuiDw"
      },
      "source": [
        "#Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usL5LynSI6rE"
      },
      "source": [
        "## Exercise 11.1:\n",
        "\n",
        "A long time ago in a galaxy far, far away, the characters of the Star Wars franchise interacted with each other in an endless series of films. An ancient Jedi order, called the *Data Guardians of the Galaxy* (not affiliated to Marvel's homonym :) registered all those interactions and saved them on a digital file so that they could be studied by the forthcoming generations. This file was originally called (guess it) `sw.txt`, and you will find it in the `/data` directory.\n",
        "\n",
        "Using pySpark, perform the following operations and answer the following questions:\n",
        "\n",
        "1. Load the `$DRIVE_DATA/sw.txt` file. Take into account that it is a JSON file.\n",
        "2. Using this information, create a graph of interactions between the Star Wars characters.\n",
        "3. How many different characters are there?\n",
        "4. How many interactions are there?\n",
        "5. Who is the central character in Star Wars (the one who interacts in most scenes)?\n",
        "6. Who is the character with the highest 'rank' in Star Wars (use the PageRank algorithm)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM--zQhLyAYM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}