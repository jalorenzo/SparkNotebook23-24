{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_12_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 12 - Exercises. Final assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za4qzjHXyxcn"
      },
      "source": [
        "## Exercise 12.1\n",
        "\n",
        "Let us extract information from the cite75_99.txt and apat63_99.txt files. Write a script that performs the following operations:\n",
        "\n",
        "1. From the cite75_99.txt file, obtain the number of citations received by each patent. You must produce a DataFrame with the following format:\n",
        "\n",
        "| PatentNum | ncitations |\n",
        "|-----------|------------|\n",
        "| 3060453   |    3       |\n",
        "| 3390168   |    6       |\n",
        "| 3626542   |   18       |\n",
        "| 3611507   |    5       |\n",
        "| 3000113   |    4       |\n",
        "\n",
        "\n",
        "2. From the apat63_99.txt file, create a DataFrame to show the patent number, its country and the patent year, discarding the rest of fields in the file. The DataFrame produced must have the following format:\n",
        "\n",
        "|PatentNum |country|Year |\n",
        "|----------|-------|-----|\n",
        "| 3070801  | BE    | 1963|\n",
        "| 3070802  | US    | 1963|\n",
        "| 3070803  | US    | 1963|\n",
        "| 3070804  | US    | 1963|\n",
        "| 3070805  | US    | 1963|\n",
        "\n",
        "\n",
        "**Requirements**\n",
        "\n",
        " - Both DataFrames must be stored in Parquet format with gzip compression. Check the number of partitions of each DataFrame and the number of files gererated.\n",
        "\n",
        " - It is **strongly advised** to copy the files from your Drive to a temporal directory in the notebook virtual machine and unzip them there. This will reduce the execution times. See the cell below:\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV_M6xMlB9hP"
      },
      "source": [
        "!mkdir /tmp/data/\n",
        "!cp \"$DRIVE_DATA\"apat63_99.txt.tar.bz2 \"$DRIVE_DATA\"cite75_99.txt.tar.bz2 /tmp/data\n",
        "%cd /tmp/data\n",
        "!ls\n",
        "!tar -jxf apat63_99.txt.tar.bz2\n",
        "!tar -jxf cite75_99.txt.tar.bz2\n",
        "!rm /tmp/data/*.tar.bz2\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF1_LXZEnzMn"
      },
      "source": [
        "## Exercise 12.2\n",
        "\n",
        "Write a code that, from the Parquet files created in the previous exercise, obtains for each country and for each year: the total number of patents, the total number of citations from those patents, the average number of citations and the maximum number of citations. Compute only those values in which there are any values in both files (*inner join*). In addition, each country must show its whole name, obtained from the *country_codes.txt* file. The final DataFrame must look like this one:\n",
        "\n",
        "\n",
        "|Country            |Year|PatentsNum |TotalCitations|AvgCitations      |MaxCitations|\n",
        "|-------------------|----|-----------|--------------|------------------|------------|\n",
        "|Algeria            |1963|2          |7             |3.5               |4           |\n",
        "|Algeria            |1968|1          |2             |2.0               |2           |\n",
        "|Algeria            |1970|1          |2             |2.0               |2           |\n",
        "|Algeria            |1972|1          |1             |1.0               |1           |\n",
        "|Algeria            |1977|1          |2             |2.0               |2           |\n",
        "|Andorra            |1987|1          |3             |3.0               |3           |\n",
        "|Andorra            |1993|1          |1             |1.0               |1           |\n",
        "|Andorra            |1998|1          |1             |1.0               |1           |\n",
        "|Antigua and Barbuda|1978|1          |6             |6.0               |6           |\n",
        "|Antigua and Barbuda|1979|1          |14            |14.0              |14          |\n",
        "|Antigua and Barbuda|1991|1          |8             |8.0               |8           |\n",
        "|Antigua and Barbuda|1994|1          |19            |19.0              |19          |\n",
        "|Antigua and Barbuda|1995|2          |12            |6.0               |11          |\n",
        "|Antigua and Barbuda|1996|2          |3             |1.5               |2           |\n",
        "|Argentina          |1963|14         |35            |2.5               |7           |\n",
        "|Argentina          |1964|20         |60            |3.0               |8           |\n",
        "|Argentina          |1965|10         |35            |3.5               |10          |\n",
        "|Argentina          |1966|16         |44            |2.75              |9           |\n",
        "|Argentina          |1967|13         |60            |4.615384615384615 |14          |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The output DataFrame must be saved in a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT3WU-1IwOPD"
      },
      "source": [
        "## Exercise 12.3\n",
        "\n",
        "From the apat63_99.txt file, obtain the number of patents per country and year **using RDDs** (do not use DataFrames). The resulting RDD must be a key/value RDD in which the key is a country and the value a list of tuples. Each tuple will be composed of a year and the number of patents of the country during that year. In addition, the resulting RDD must be sorted by  the country code and, for each country, values must be sorted by year.\n",
        "\n",
        "Example of output key/value entry:\n",
        "\n",
        "    (u'PA', [(u'1963', 2), (u'1964', 2), (u'1965', 1), (u'1966', 1), (u'1970', 1), (u'1971', 1), (u'1972', 6), (u'1974', 3), (u'1975', 5), (u'1976', 3), (u'1977', 2), (u'1978', 2), (u'1980', 2), (u'1982', 1), (u'1983', 1), (u'1985', 2), (u'1986', 1), (u'1987', 2), (u'1988', 1), (u'1990', 1), (u'1991', 2), (u'1993', 1), (u'1995', 1), (u'1996', 1), (u'1999', 1)])\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- You must remove the double quotation marks from the country code.\n",
        "- Use 8 partitions to read the apat63_99.txt.bz2 file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0LiWoc4VQdh"
      },
      "source": [
        "## Exercise 12.4\n",
        "\n",
        "From the Parquet files created in Exercise 12.1, create a DataFrame that gives the patent or patents with the higher number of citations per country and year, as well as the average of the number of citations per country and year, and the difference between the maximum and the average values. The resulting DataFrame should look like this:\n",
        "\n",
        "\n",
        "|Country|Year|PatentNum|max  |average       |diff              |\n",
        "|-------|----|---------|-----|--------------|------------------|\n",
        "|AD     |1987|4688621  |3    |3.0           |0.0               |\n",
        "|AD     |1993|5193231  |1    |1.0           |0.0               |\n",
        "|AD     |1998|5765303  |1    |1.0           |0.0               |\n",
        "|AE     |1984|4482959  |5    |5.0           |0.0               |\n",
        "|AE     |1985|4554981  |14   |14.0          |0.0               |\n",
        "|AE     |1987|4663181  |3    |3.0           |0.0               |\n",
        "|AE     |1989|4805221  |7    |5.0           |2.0               |\n",
        "|AE     |1990|4909321  |2    |2.0           |0.0               |\n",
        "|AE     |1991|5004552  |3    |2.0           |1.0               |\n",
        "|AE     |1992|5104556  |4    |4.0           |0.0               |\n",
        "|AE     |1993|5181569  |8    |8.0           |0.0               |\n",
        "|AE     |1996|5580125  |1    |1.0           |0.0               |\n",
        "|AG     |1978|4126850  |6    |6.0           |0.0               |\n",
        "|AG     |1979|4172981  |14   |14.0          |0.0               |\n",
        "|AG     |1991|5013035  |8    |8.0           |0.0               |\n",
        "|AG     |1994|5345071  |19   |19.0          |0.0               |\n",
        "|AG     |1995|5457307  |11   |6.0           |5.0               |\n",
        "|AG     |1996|5525786  |2    |1.5           |0.5               |\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektPyLWzaImT"
      },
      "source": [
        "## Exercise 12.5\n",
        "\n",
        "From the Parquet file with the (PatentNum,Country,Year) information from Exercise 12.1, create a DataFrame that shows the number of patents associated to each country per decade (understanding as a *decade* the years from 0 to 9; e.g. from 1970 to 1979). In addition, the DataFrame must show the increase or decrease of the number of patents per country and decade with respect to the previous decade. The resulting DataFrame must look like this:\n",
        "\n",
        "|Country|Decade|PatentsNum|Diff|\n",
        "|-------|------|----------|----|\n",
        "|AD     |1980  |1         |0   |\n",
        "|AD     |1990  |5         |4   |\n",
        "|AE     |1980  |7         |0   |\n",
        "|AE     |1990  |11        |4   |\n",
        "|AG     |1970  |2         |0   |\n",
        "|AG     |1990  |7         |5   |\n",
        "|AI     |1990  |1         |0   |\n",
        "|AL     |1990  |1         |0   |\n",
        "|AM     |1990  |2         |0   |\n",
        "|AN     |1970  |1         |0   |\n",
        "|AN     |1980  |2         |1   |\n",
        "|AN     |1990  |5         |3   |\n",
        "|AR     |1960  |135       |0   |\n",
        "|AR     |1970  |239       |104 |\n",
        "|AR     |1980  |184       |-55 |\n",
        "|AR     |1990  |292       |108 |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression."
      ]
    }
  ]
}