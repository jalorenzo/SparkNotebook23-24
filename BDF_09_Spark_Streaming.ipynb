{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_09_Spark_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 09 - Spark Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trU_wEl7Og5p"
      },
      "source": [
        "-   Scalable, *high-throughput*, fault-tolerant streaming processing\n",
        "\n",
        "<img src=\"https://4.bp.blogspot.com/-HJ8x45gN3kE/WNCULL6J6eI/AAAAAAAAAZ0/1LfYt5IwE3sINSxWunqTrbqyrm7irZTCwCEw/s1600/spark.JPG\" alt=\"Spark Streaming flow\" style=\"width: 900px;\"/>\n",
        "\n",
        "-   Input from multiple sources: Kafka, Flume, Twitter, ZeroMQ, Kinesis or sockets TCP\n",
        "\n",
        "\n",
        "## Spark Streaming APIs\n",
        "\n",
        "- DStream API\n",
        "      - Original API, based on RDDs\n",
        "- Structured Streaming\n",
        "      - Available from version 2.2, based on DataFrames\n",
        "\n",
        "\n",
        "Spark Streaming page: <https://spark.apache.org/streaming/>\n",
        "Main documentation (last version): <https://spark.apache.org/docs/latest/streaming-programming-guide.html>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of20h1csKttE"
      },
      "source": [
        "\n",
        "## DStream API\n",
        "\n",
        "Main abstraction: DStream (`discretized stream`).\n",
        "\n",
        "-   Represents a continuous data stream\n",
        "\n",
        "![dstreams](http://persoal.citius.usc.es/tf.pena/TCDM/figs/dstreams.png)\n",
        "\n",
        "*Micro-batch* architecture\n",
        "\n",
        "-   Received data are grouped into batches\n",
        "-   Batches are created at regular intervals (batch interval)\n",
        "-   Every batch has the form of an RDD, which can be processed by Spark\n",
        "-   In addition, stateful transformations can be performed by\n",
        "    -   Window operations\n",
        "    -   Tracking of each key state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLHkK1rZMiTH"
      },
      "source": [
        "### DStream example: stateless online WordCount example\n",
        "\n",
        "To run the example:\n",
        "\n",
        "   1. In a terminal, access the Docker container with `docker exec -ti container_id /bin/bash` (run `docker ps` to know the container_id)\n",
        "   2. Once in the container's terminal, use netcat as a server in the port 9999\n",
        "\n",
        "    `$ nc -lk 9999`\n",
        "\n",
        "   2. Run here the PySpark code that you will find below\n",
        "\n",
        "   3. Write text lines in netcat's terminal. They will be picked up and processed by the script\n",
        "    - Write repeated words, to make sure they are counted right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycdjim6PN9Oc"
      },
      "source": [
        "!sudo apt-get update && sudo apt-get install netcat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev0hpG2aOxi7"
      },
      "source": [
        "!nohup nc -lk 9999 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Bg2GybM06T"
      },
      "source": [
        "from pyspark.streaming import StreamingContext\n",
        "from operator import add\n",
        "\n",
        "sc.setLogLevel(\"WARN\")\n",
        "\n",
        "# Streaming context with a batch interval of 5 sec.\n",
        "ssc = StreamingContext(sc, 5)\n",
        "\n",
        "# DStream that connects to localhost:9999\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Run a WordCount\n",
        "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
        "              .map(lambda word: (word, 1))\\\n",
        "              .reduceByKey(add)\n",
        "\n",
        "counts.pprint()\n",
        "\n",
        "ssc.start() # Start the computation\n",
        "ssc.awaitTerminationOrTimeout(60) # Wait for it to finish (stops in 60 seconds)\n",
        "ssc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W5bT_xmNAwB"
      },
      "source": [
        "### DStream example: stateful online WordCount example\n",
        "\n",
        "Repeat the previous steps running the following code\n",
        "\n",
        " - Check that the number of words is accumulated between accesses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2i1wCu9NKJn"
      },
      "source": [
        "from pyspark.streaming import StreamingContext\n",
        "from operator import add\n",
        "\n",
        "sc.setLogLevel(\"WARN\")\n",
        "\n",
        "# Streaming context with a batch interval of 5 sec.\n",
        "ssc = StreamingContext(sc, 5)\n",
        "\n",
        "# DStream that connects to localhost:9999\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "ssc.checkpoint(\"/tmp/cpdir\") # Enables checkpoint\n",
        "\n",
        "def updateFunc(new_values, last_sum):\n",
        "    return sum(new_values) + (last_sum or 0)\n",
        "\n",
        "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
        "              .map(lambda word: (word, 1))\\\n",
        "              .updateStateByKey(updateFunc)\n",
        "\n",
        "counts.pprint()\n",
        "\n",
        "ssc.start() # Start the computation\n",
        "ssc.awaitTerminationOrTimeout(60) # Wait for it to finish (stops in 60 seconds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFwgHSUDNPUZ"
      },
      "source": [
        "## Structured Streaming\n",
        "\n",
        "Utilises the structured API (DataFrames, DataSets and SQL)\n",
        "\n",
        "- As they arrive to the system, it reads data, processes them and adds them to a DataFrame\n",
        "\n",
        "Input sources:\n",
        "\n",
        "- [Apache Kafka](https://kafka.apache.org/)\n",
        "- Files (by continuously reading files from a directory\n",
        "- Sockets\n",
        "\n",
        "Sinks (data destination):\n",
        "\n",
        "- Apache Kafka\n",
        "- Files\n",
        "- Other computations\n",
        "- Memory (for debugging and testing)\n",
        "\n",
        "Still under development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdV_vbTENWu2"
      },
      "source": [
        "### Example: Process files in the  $DRIVE_DATA/by-day/ directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azWfdEFP4eV6"
      },
      "source": [
        "!ls \"$DRIVE_DATA/by-day\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVA5zSa9Nd8O"
      },
      "source": [
        "# Check the format of a file\n",
        "!head \"$DRIVE_DATA\"/by-day/2010-12-01.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-u70JjDNf83"
      },
      "source": [
        "# Create a DataFrame containing the data from one of the files\n",
        "dfStatic = spark.read\\\n",
        "                  .format(\"csv\")\\\n",
        "                  .option(\"header\", \"true\")\\\n",
        "                  .option(\"inferSchema\", \"true\")\\\n",
        "                  .load(os.environ[\"DRIVE_DATA\"]+\"/by-day/2010-12-01.csv\")\n",
        "dfStatic.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPZKUFTBNh3F"
      },
      "source": [
        "# Obtain a DataFrame containing the purchases per hour and per client throughout that day\n",
        "from pyspark.sql.functions import window, col, desc\n",
        "purchasePerClientPerHourStatic =\\\n",
        "             dfStatic.select(\n",
        "                                col(\"CustomerId\"),\n",
        "                                (col(\"UnitPrice\")*col(\"Quantity\")).alias(\"total_cost\"),\n",
        "                                col(\"InvoiceDate\"))\\\n",
        "                       .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
        "                       .sum(\"total_cost\")\n",
        "\n",
        "purchasePerClientPerHourStatic.show(15, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdANptAtNkSg"
      },
      "source": [
        "# Because of the shuffling, the number of partitions is quite large\n",
        "print(purchasePerClientPerHourStatic.rdd.getNumPartitions())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYaCT4LbNmpM"
      },
      "source": [
        "# Change the number of partitions to use and create the DataFrame again\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
        "purchasePerClientPerHourStatic =\\\n",
        "                            dfStatic.select(\n",
        "                                col(\"CustomerId\"),\n",
        "                                (col(\"UnitPrice\")*col(\"Quantity\")).alias(\"total_cost\"),\n",
        "                                col(\"InvoiceDate\"))\\\n",
        "                            .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
        "                            .sum(\"total_cost\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0hJRMM1NnTo"
      },
      "source": [
        "print(purchasePerClientPerHourStatic.rdd.getNumPartitions())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw4fanywNq0K"
      },
      "source": [
        "# Define a DataFrame in Streaming that takes as a data source\n",
        "# the files in the $DRIVE_DATA/by-day/ directory.\n",
        "# Set it to read 1 file each time it is triggered\n",
        "dfStreaming = spark.readStream\\\n",
        "                   .schema(dfStatic.schema)\\\n",
        "                   .option(\"maxFilesPerTrigger\", 1)\\\n",
        "                   .format(\"csv\")\\\n",
        "                   .option(\"header\", \"true\")\\\n",
        "                   .load(os.environ[\"DRIVE_DATA\"]+\"/by-day/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n1XQiAwNtWF"
      },
      "source": [
        "# From the previous Streaming DataFrame, we obtain the purchases per hour and per client\n",
        "purchasePerClientPerHourStreaming = \\\n",
        "            dfStreaming.select(\n",
        "                               col(\"CustomerId\"),\n",
        "                              (col(\"UnitPrice\")*col(\"Quantity\")).alias(\"total_cost\"),\n",
        "                               col(\"InvoiceDate\"))\\\n",
        "                       .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
        "                       .sum(\"total_cost\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGdyO44Nvub"
      },
      "source": [
        "# Create a DataStreamWriter object to write the values of the previous DataFrame\n",
        "# Values are written to a in-memory table\n",
        "# The writing mode is \"complete\": the whole output is overwritten\n",
        "# Data can be accessed using the purchases_per_hour table\n",
        "# Data from the input are read every second\n",
        "lookupPurchases = purchasePerClientPerHourStreaming\\\n",
        "                    .writeStream\\\n",
        "                    .format(\"memory\")\\\n",
        "                    .queryName(\"purchases_per_hour\")\\\n",
        "                    .outputMode(\"complete\")\\\n",
        "                    .trigger(processingTime='1 seconds')\n",
        "print(type(lookupPurchases))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfbasRomNy3y"
      },
      "source": [
        "# Methods defined for a DataStreamWriter\n",
        "[method_name for method_name in dir(lookupPurchases)\n",
        " if callable(getattr(lookupPurchases, method_name))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TijNjfeN012"
      },
      "source": [
        "# Start access to the input data\n",
        "lookupPurchases.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRt0TIbmN3EF"
      },
      "source": [
        "# Start showing each second the content of the table\n",
        "from time import sleep\n",
        "for x in range(20):\n",
        "    spark.sql(\"\"\"\n",
        "            SELECT *\n",
        "            FROM purchases_per_hour\n",
        "            ORDER BY `sum(total_cost)` DESC\n",
        "            \"\"\").show(15, truncate=False)\n",
        "    sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}