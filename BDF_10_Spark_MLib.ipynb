{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_10_Spark_MLib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 10 - Spark MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vCoMnv84_Hc"
      },
      "source": [
        "Library of ML parallel algorithms for massive data\n",
        "\n",
        "-   Machine learning classic algorithms: classification, regression, clustering, collaborative filtering\n",
        "-   Other algorithms: feature extraction, transformation, dimensionality reduction, and selection\n",
        "-   Tools to build, evaluate and adjust ML pipelines\n",
        "-   Other tools: linear algebra, statistics, data processing, etc.\n",
        "\n",
        "\n",
        "Two packages:\n",
        "\n",
        "-   **spark.mllib:** Original RDD-based API\n",
        "-   **spark.ml:** High-level API, based on DataFrames\n",
        "\n",
        "Documentation and APIS:\n",
        "\n",
        "- ML\n",
        "    - Guia: http://spark.apache.org/docs/latest/ml-guide.html\n",
        "    - API Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\n",
        "    - API Scala: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.package\n",
        "- MLlib\n",
        "    - Guia: http://spark.apache.org/docs/latest/mllib-guide.html\n",
        "    - API Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.mllib.html\n",
        "    - API Scala: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.package\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWMYgbMB5mLS"
      },
      "source": [
        "## Example\n",
        "\n",
        "Use the [KMeans](http://spark.apache.org/docs/latest/mllib-clustering.html#k-means) clustering algorithm to group data from vectors spread over two clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg-_EowF52_0"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans, KMeansModel\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "#  Define an array of 4 sparse vectors, 3 elements each\n",
        "sparseData = [\n",
        "     Vectors.sparse(3, {1: 1.2}),\n",
        "     Vectors.sparse(3, {1: 1.1}),\n",
        "     Vectors.sparse(3, {0: 0.9, 2: 1.0}),\n",
        "     Vectors.sparse(3, {0: 1.0, 2: 1.1})\n",
        " ]\n",
        "\n",
        "for i in range(4):\n",
        "    print(sparseData[i].toArray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FIwTWDw6Mzx"
      },
      "source": [
        "# Turn the array into a DataFrame\n",
        "dfSD = sc.parallelize([\n",
        "  (1, sparseData[0]),\n",
        "  (2, sparseData[1]),\n",
        "  (3, sparseData[2]),\n",
        "  (4, sparseData[3])\n",
        "]).toDF([\"row\", \"features\"])\n",
        "\n",
        "dfSD.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9_4SXD7AqG"
      },
      "source": [
        "# Create a KMeans model without training, with 2 clusters\n",
        "# For more information, see https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#module-pyspark.ml.clustering\n",
        "kmeans = KMeans()\\\n",
        "    .setInitMode(\"k-means||\")\\\n",
        "    .setFeaturesCol(\"features\")\\\n",
        "    .setPredictionCol(\"prediction\")\\\n",
        "    .setK(2)\\\n",
        "    .setSeed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpfUDkqA9tju"
      },
      "source": [
        "# Adjust the model to the previous DataFrame and show the cluster centres\n",
        "kmModel = kmeans.fit(dfSD)\n",
        "print(\"Clusters centres: {0}\".format(\n",
        "    kmModel.clusterCenters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyqXUOLk-B-X"
      },
      "source": [
        "# Verify that the model clusters the data from the previous array\n",
        "kmModel.transform(dfSD).show()\n",
        "# Calculate the cost as the addition of the squared distance between the input points\n",
        "# and the centres of the corresponding clusters\n",
        "print(\"Cost = {0}\".format(\n",
        "    kmModel.summary.trainingCost))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG_qkJlC-ogG"
      },
      "source": [
        "# Test the model with other points\n",
        "dfTest = sc.parallelize([\n",
        "  (1, Vectors.sparse(3, {0: 0.9, 1:1.0, 2: 1.0})),\n",
        "  (2, Vectors.sparse(3, {1: 1.5, 2: 0.3}))\n",
        "]).toDF([\"row\", \"features\"])\n",
        "\n",
        "kmModel.transform(dfTest).show(truncate=False)\n",
        "\n",
        "# Calculate the cost as the addition of the squared distance between the input points\n",
        "# and the centres of the corresponding clusters\n",
        "print(\"Cost = {0}\".format(\n",
        "    kmModel.summary.trainingCost))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNwDiJtmBhtP"
      },
      "source": [
        "# Save the model in a directory\n",
        "kmModel.save(\"/tmp/kmModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y23ahGDqBolO"
      },
      "source": [
        "# Reload the model\n",
        "sameModel = KMeansModel.load(\"/tmp/kmModel\")\n",
        "\n",
        "sameModel.transform(dfTest).show(truncate=False)\n",
        "# Calculate the cost as the addition of the squared distance between the input points\n",
        "# and the centres of the corresponding clusters\n",
        "# print(\"Cost = {0}\".format(sameModel.summary.trainingCost))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnuGHaLXB3fL"
      },
      "source": [
        "!rm -rf /tmp/kmModel"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}