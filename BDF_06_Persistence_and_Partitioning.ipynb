{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_06_Persistence_and_Partitioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 06 - Persistence and Partitioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94O191-heh5o"
      },
      "source": [
        "We will show here two important aspects of Apache Spark\n",
        "\n",
        "- `Persistence`: how to store DataFrames and RDDs in a way so that they do not need to be recalculated\n",
        "- `Partitioning`:  how to specify and change the partitions of a DataFrame or RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOH011r_e5iz"
      },
      "source": [
        "## Persistence\n",
        "\n",
        "Issues when reusing an RDD several times:\n",
        "\n",
        "-   Spark recalculates the RDD as well as its dependencies every time an action is executed\n",
        "-   Very costly (particularly in iterative problems)\n",
        "\n",
        "Solution\n",
        "\n",
        "-   Keep the RDD in memory and/or disk\n",
        "-   Use `cache()` or `persist()` methods\n",
        "\n",
        "### Persistence levels (as defined in [`pyspark.StorageLevel`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html#pyspark.StorageLevel) and [`org.apache.spark.storage.StorageLevel`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.storage.StorageLevel))\n",
        " Level                | Space used  | CPU time     | Memory/Disk   | Comments\n",
        " :------------------: | :------:    | :-----:      | :-------------: | ------------------\n",
        " MEMORY_ONLY          |   High      |   Low        |     Memory   | Stores the RDD in the JVM as a non-serialised Java object. If the RDD does not fit in memory, some partitions will not be cached in memory and will be recalculated on the fly every time they are required. Default level in Java and Scala.\n",
        " MEMORY_ONLY_SER      |   Low       |   High       |     Memory   | Stores the RDD as a serialised Java object (a *byte array* per partition). Default level in Python, using [`pickle`](http://docs.python.org/2/library/pickle.html).\n",
        " MEMORY_AND_DISK      |   High      |   Medium     |     Both     | Stores the RDD in the JVM as a non-serialised Java object. If the RDD does not fit in memory, the partitions that do not fit will be spilled to disk and read from there every time they are required.\n",
        " MEMORY_AND_DISK_SER  |   Low       |   High       |     Both     | Similar to MEMORY_AND_DISK but using serialised objects.\n",
        " DISK_ONLY            |   Low       |   High       |     Disk     | Stores the RDD partitions only on disk.\n",
        " OFF_HEAP             |   Low       |   High       |   Memory     | Stores the serialised RDD using *off-heap* memory (outside the JVM's heap) which can reduce the overhead of the garbage collector.\n",
        "   \n",
        "\n",
        "\n",
        "    \n",
        "### Persistence levels\n",
        "\n",
        "-   In Scala and Java, the default level is MEMORY\\_ONLY\n",
        "\n",
        "-   In Python, data are always serialised (by default as *pickled* objects)\n",
        "\n",
        "    - MEMORY_ONLY and MEMORY_AND_DISK levels are equivalent to MEMORY_ONLY_SER and MEMORY_AND_DISK_SER\n",
        "    - When creating the SparkContext it is possible to request a serialisation [`marshal`](https://docs.python.org/2/library/marshal.html#module-marshal)\n",
        "    \n",
        "```python\n",
        "sc = SparkContext(master=\"local\", appName=\"My app\", serializer=pyspark.MarshalSerializer())\n",
        "```\n",
        "    \n",
        "### Fault tolerance\n",
        "\n",
        "-   If a node with stored data fails, the RDD is recomputed\n",
        "\n",
        "    -   Adding `_2` to the persistence level, 2 copies of the RDD are stored\n",
        "        \n",
        "### Cache management\n",
        "\n",
        "-   LRU algorithm to manage the cache memory\n",
        "\n",
        "    -   For *only memory* levels, the old RDDs are deleted and recalculated\n",
        "    -   For *memory and disk* levels, partitions that do not fit in memory are spilled to disk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZBQ1uQiIKV"
      },
      "source": [
        "### Persistence with DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw_w69siiPBT"
      },
      "source": [
        "dfFlightsData = (spark\n",
        "    .read\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .csv(os.environ[\"DRIVE_DATA\"] + \"2015-summary.csv\"))\n",
        "print(\"Cached: {0}\".format(dfFlightsData.is_cached))\n",
        "print(\"Level without persistence: {0}\".format(dfFlightsData.storageLevel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-T_B6PFiyIf"
      },
      "source": [
        "dfFlightsData.cache()\n",
        "print(\"Cached: {0}\".format(dfFlightsData.is_cached))\n",
        "print(\"Persistence level by default: {0}\".format(dfFlightsData.storageLevel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4O3d_EYjC9h"
      },
      "source": [
        "# To chanche the persistence level, we need first to remove it from cache\n",
        "dfFlightsData.unpersist()\n",
        "\n",
        "from pyspark import StorageLevel\n",
        "dfFlightsData.persist(StorageLevel.MEMORY_ONLY_2)\n",
        "print(\"Cached: {0}\".format(dfFlightsData.is_cached))\n",
        "print(\"New persistence level: {0}\".format(dfFlightsData.storageLevel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb8hwepbdys2"
      },
      "source": [
        "# Persistence is not inherited in transformations\n",
        "dfData2 = dfFlightsData.select(\"DEST_COUNTRY_NAME\")\n",
        "print(\"Cached: {0}\".format(dfData2.is_cached))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbon05TIeoFF"
      },
      "source": [
        "### Persistence with RDDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLkFa5lkfYNp"
      },
      "source": [
        "rdd = sc.parallelize(range(1000), 10)\n",
        "print(\"Cached: {0}\".format(rdd.is_cached))\n",
        "print(\"Level without persistence: {0}\".format(rdd.getStorageLevel()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ihPiyKf_2X"
      },
      "source": [
        "rdd.cache()\n",
        "print(\"Cached: {0}\".format(rdd.is_cached))\n",
        "print(\"Default persistence level: {0}\".format(rdd.getStorageLevel()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJJV7LmHgFJg"
      },
      "source": [
        "# Take rdd out of the cache memory\n",
        "rdd.unpersist()\n",
        "\n",
        "from pyspark import StorageLevel\n",
        "rdd.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
        "print(\"Cached: {0}\".format(rdd.is_cached))\n",
        "print(\"New persistence level: {0}\".format(rdd.getStorageLevel()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEDSqbjWgLdF"
      },
      "source": [
        "# Persistence is not inherited in transformations\n",
        "rdd2 = rdd.map(lambda x: x*x)\n",
        "print(\"Cached: {0}\".format(rdd2.is_cached))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLPVAnFoiOmr"
      },
      "source": [
        "### Checkpointing with RDDs\n",
        "RDDs can be checkpointed, forcing them to be stored on disk.\n",
        "\n",
        "- I is a *lazy* operation: data are not stored on disk until an Action is dispatched\n",
        "- Future references to those RDDs will load them from disk instead of recomputing them\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olKUeTmWkzuI"
      },
      "source": [
        "!mkdir -p \"$DRIVE_DATA\"/CP\n",
        "!ls \"$DRIVE_DATA\"\n",
        "#just in case...\n",
        "#rm -rf \"$DRIVE_DATA\"/CP/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDICH0GDlcqc"
      },
      "source": [
        "rdd = sc.parallelize(range(100000))\n",
        "spark.sparkContext.setCheckpointDir(os.environ[\"DRIVE_DATA\"] + \"CP\")\n",
        "rdd.checkpoint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vAehUbdlqjw"
      },
      "source": [
        "!ls -lR \"$DRIVE_DATA\"CP/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXAQYrxCl3c6"
      },
      "source": [
        "rdd.count()\n",
        "!ls -lR \"$DRIVE_DATA\"CP/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4WZkbtql4QE"
      },
      "source": [
        "!rm -rf \"$DRIVE_DATA\"CP/\n",
        "!ls -lR \"$DRIVE_DATA\"CP/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_MNCZhFSiNy"
      },
      "source": [
        "## Partitioning\n",
        "\n",
        "The number of partitions is a function of the cluster size or the number of blocks of the HDFS file\n",
        "\n",
        "-   It can be adjusted when creating or operating on an RDD\n",
        "    \n",
        "    - RDDs offer a greater control on their partitioning\n",
        "\n",
        "-   For DataFrames it is possible to modify it once created.\n",
        "\n",
        "-   The parallelism of RDDs derived from other ones depends on their parent's.\n",
        "\n",
        "-   Useful properties:\n",
        "    -    `spark.default.parallelism` For RDDs, numbre of partitions by default returned by default by transformations like parallelize, join and reduceByKey\n",
        "        - Fixed value for a SparkContext\n",
        "        - The property `sc.defaultParallelism` indicates its value\n",
        "    -    `spark.sql.shuffle.partitions` For DataFrames, number of partitions to use when using data in *wide* transformations\n",
        "        - It can be modified using `spark.conf.set`\n",
        "\n",
        "- Useful functions:\n",
        "    -   `rdd.getNumPartitions()` returns the number of partitions of the RDD\n",
        "    -   `rdd.glom()` returns a new RDD joining the elements on each partition into a list\n",
        "\n",
        "    - `repartition(n)` returns a new DataFrame or RDD with exactly `n` partitions\n",
        "    - `coalesce(n)` optimised version of `repartition`, allows avoiding data movement\n",
        "        - But only if you are decreasing the number of partitions.\n",
        "    - `partitionBy(n,[partitionFunc])` Partitioning by key, using a partitioning function (by default, a hash of the key)\n",
        "        - Only for key/value RDDs\n",
        "        - Ensures that pairs with the same key go to the same partition\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYZ_QWpDtIhr"
      },
      "source": [
        "### Partitions and RDDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRlo8ChDtVqv"
      },
      "source": [
        "print(\"Number of partitions by default for RDDs: {0}\"\n",
        "       .format(sc.defaultParallelism))\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 2, 4, 1], 2)\n",
        "pairs = rdd.map(lambda x: (x, x*x))\n",
        "\n",
        "print(\"RDD pairs = {0}\".format(pairs.collect()))\n",
        "print(\"Pairs partitioning: {0}\".format(pairs.glom().collect()))\n",
        "print(\"Number of pair partitions = {0}\".format(pairs.getNumPartitions()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c82bElrJtYGd"
      },
      "source": [
        "# Reduction keeping the number of partitions\n",
        "from operator import add\n",
        "print(\"Reduction keeping partitions: {0}\".format(\n",
        "        pairs.reduceByKey(add).glom().collect()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v17olWQHtawC"
      },
      "source": [
        "# Reduction modifying the number of partitions\n",
        "print(\"Reduction with 3 partitions: {0}\".format(\n",
        "       pairs.reduceByKey(add, 3).glom().collect()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJiIKnrctdAM"
      },
      "source": [
        "# Repartitions example\n",
        "pairs4 = pairs.repartition(4)\n",
        "print(\"pairs4 with {0} partitions: {1}\".format(\n",
        "        pairs4.getNumPartitions(),\n",
        "        pairs4.glom().collect()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np0Zbz9Tth7_"
      },
      "source": [
        "# Coalesce example\n",
        "pairs2 = pairs4.coalesce(2)\n",
        "print(\"pairs2 with {0} partitions: {1}\".format(\n",
        "        pairs2.getNumPartitions(),\n",
        "        pairs2.glom().collect()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9iiePlttqbW"
      },
      "source": [
        "# Partitioning by key\n",
        "pairs_key = pairs2.partitionBy(4)\n",
        "print(\"Partitions by key ({0} partitions): {1}\".format(\n",
        "        pairs_key.getNumPartitions(),\n",
        "        pairs_key.glom().collect()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_9ibNU4ttCy"
      },
      "source": [
        "# Using a partitioning function\n",
        "def partitionEvenOdd(key):\n",
        "    if key%2:\n",
        "        return 0  # Odd keys go to partition 0\n",
        "    else:\n",
        "        return 1  # Even keys go to partition 1\n",
        "\n",
        "pairs_evenodd = pairs2.partitionBy(2, partitionEvenOdd)\n",
        "print(\"Partition by key ({0} partitions): {1}\".format(\n",
        "        pairs_evenodd.getNumPartitions(),\n",
        "        pairs_evenodd.glom().collect()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej832t8ZtvyJ"
      },
      "source": [
        "### Partitions and DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LErImSLruBNf"
      },
      "source": [
        "# Convert the RDD to a DataFrame\n",
        "dfPairs = pairs.toDF()\n",
        "dfPairs.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le5CphstuYbb"
      },
      "source": [
        "# The DataFrame inherits the number of partitions from the RDD\n",
        "print(\"Number of partitions of the DataFrame: {0}\"\n",
        "      .format(dfPairs.rdd.getNumPartitions()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc1ackRUuatC"
      },
      "source": [
        "# A narrow transformation keeps the number of partitions\n",
        "print(\"Number of partitions after a narrow transformation: {0}\"\n",
        "      .format(dfPairs.replace(1, 2).rdd.getNumPartitions()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmsrd1e1ue3Z"
      },
      "source": [
        "# A wide transformation does not keep the number of partitions\n",
        "print(\"Number of partitions after a wide transformation: {0}\"\n",
        "      .format(dfPairs.sort(\"_1\").rdd.getNumPartitions()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsDOxbhuuhv6"
      },
      "source": [
        "# It is possibe to specify the number of partitions to use in the wide transformation\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 2)\n",
        "print(\"Number of partitions after a wide transformation: {0}\"\n",
        "      .format(dfPairs.sort(\"_1\").rdd.getNumPartitions()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8AASwLXuqUf"
      },
      "source": [
        "## Working at partition level\n",
        "\n",
        "A `map`  operation is applied to each element of the RDD (or a `foreach` for each row of the DataFrame)\n",
        "\n",
        "-  It may imply redundant operations (f.ex. opening a connection to a DB)\n",
        "\n",
        "-  It may not be very efficient\n",
        "\n",
        "`map` and `foreach` can be called once per partition:\n",
        "\n",
        "-   Methods `mapPartitions()`, `mapPartitionsWithIndex()` and `foreachPartition()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBKnTr8vu0nr"
      },
      "source": [
        "nums = sc.parallelize([1,2,3,4,5,6,7,8,9], 4)\n",
        "print(nums.glom().collect())\n",
        "\n",
        "def addAndCount(iterator):\n",
        "    addCount = [0,0]\n",
        "    for i in iterator:\n",
        "        addCount[0] += i\n",
        "        addCount[1] += 1\n",
        "    return addCount\n",
        "\n",
        "# Call the addAndCount function once per partition\n",
        "# The iterator includes the values of the partition\n",
        "print(nums.mapPartitions(addAndCount).glom().collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRmwCVh-u3FX"
      },
      "source": [
        "def addAndCountIndex(index, iterator):\n",
        "    return \"Partition \"+str(index), addAndCount(iterator)\n",
        "\n",
        "# index is the number of partition\n",
        "print(nums.mapPartitionsWithIndex(addAndCountIndex).glom().collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM5gMbebu5Uc"
      },
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "def f(iterator):\n",
        "    tempfich, tempname = tempfile.mkstemp(dir=tempdir,text=True)\n",
        "    for x in iterator:\n",
        "        print(list(iterator)[0])\n",
        "        os.write(tempfich, (str(x)+'\\t').encode())\n",
        "    os.close(tempfich)\n",
        "\n",
        "tempdir = \"/tmp/foreachPartition\"\n",
        "\n",
        "if not os.path.exists(tempdir):\n",
        "    os.mkdir(tempdir)\n",
        "    # For each partition of the RDD, a temporary file is created.\n",
        "    # The values of the partition are written to that file.\n",
        "    nums.foreachPartition(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puOkWKIqV906"
      },
      "source": [
        "!ls -al /tmp/foreachPartition"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}