{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_01_Introduction_to_Apache_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-h_wDcNlH_K"
      },
      "source": [
        "#01 - Introduction to Apache Spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us4U59JalbaW"
      },
      "source": [
        "## A fast cluster computing platform\n",
        "\n",
        "-   It extends the MapReduce model to support efficiently other computing types\n",
        "\n",
        "    -   Interactive queries\n",
        "\n",
        "    -   Streaming processing\n",
        "\n",
        "-   Supports in-memory computations\n",
        "\n",
        "-   Surpasses MapReduce for complex operations (10-20x faster)\n",
        "\n",
        "### General-purpose\n",
        "\n",
        "-   Batch, interactive or streaming processing modes\n",
        "\n",
        "-   Reduces the number of tools to use and maintain\n",
        "\n",
        "\n",
        "### History\n",
        "\n",
        "-   Started in 2009 in the UC Berkeley RAD Lab (AMPLab)\n",
        "\n",
        "    -   Motivated by MapReduce's lack of efficiency for iterative and interactive jobs\n",
        "\n",
        "-   Main contributors: [Databricks](https://databricks.com/), Yahoo! and Intel\n",
        "\n",
        "-   Licensed as an open source project in March 2010\n",
        "\n",
        "-   Transferred to the Apache Software Foundation in June 2013, Top Level Project in February 2014\n",
        "\n",
        "-   One of the most active Big Data projects\n",
        "\n",
        "-   Version 1.0 launched in May 2014"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCqXHQnKl4Rd"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Spark features\n",
        "\n",
        "- It supports a variety of workloads: batch, interactive queries, streaming, machine learning, graph processing...\n",
        "- APIs in Scala, Java, Python, SQL and R\n",
        "- Interactive shells in Scala and Python\n",
        "- Integrates smoothly with other Big Data solutions like HDFS, Cassandra, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dKaEo3PmGdq"
      },
      "source": [
        "## The Spark stack\n",
        "<hr />\n",
        "\n",
        "![sparkstack](https://www.oreilly.com/library/view/learning-spark/9781449359034/assets/lnsp_0101.png)\n",
        "\n",
        "Source: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O'Reilly, 2015\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s01bbZxyK1oR"
      },
      "source": [
        "## Spark Core APIs\n",
        "\n",
        "Spark provides two APIs:\n",
        "\n",
        " - High-level or structured API\n",
        " - Low-level API\n",
        "\n",
        "Each API provides different data types:os:\n",
        "\n",
        " - The structured API is preferable for its better performance\n",
        " - The low-level API allows for a better control on how data are distributed\n",
        " - The high-level API uses the low-level primitives\n",
        "\n",
        "## Structured API data types\n",
        "\n",
        "### DataSets\n",
        "Distributed collection of same-type objects\n",
        "\n",
        "- Introduced on Spark > 1.6\n",
        "- The DataSets API is only available on Scala and Java\n",
        "- Not available on Python nor R because of the dynamic type nature of these languages\n",
        "\n",
        "### DataFrames\n",
        "A DataFrame is a DataSet organised in named columns\n",
        "\n",
        "- Conceptually is like a table  in a database or a dataframe in Python Pandas or R.\n",
        "- API available on Scala, Java, Python and R\n",
        "- On [Java](http://spark.apache.org/docs/latest/api/java/index.html \"Interface Row\") and [Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row \"trait Row extends Serializable\"), a DataFrame is a DataSet of objects of *Row* type\n",
        "\n",
        "\n",
        "## Low-level API data types\n",
        "### RDDs (Resilient Distributed Datasets)\n",
        "\n",
        "It is a distributed list of objects\n",
        "- It is the basic data type on Spark v1.X\n",
        "-We will be working on Spark v2.X\n",
        "\n",
        "\n",
        "## Better performance of the structured API\n",
        "\n",
        "- Spark with DataFrames and DataSets takes advantage of the structured data to improve the performance by using the  [Catalyst](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html \"Deep Dive into Spark SQLâ€™s Catalyst Optimizer\")  Optimizer, a query optimiser and the run engine  [Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html \"Project Tungsten: Bringing Apache Spark Closer to Bare Metal\").\n",
        "\n",
        "<img src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\" alt=\"Performance improvement\" style=\"width: 650px;\"/>\n",
        "\n",
        "Source: [Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html \"Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "758t9wXkoT5P"
      },
      "source": [
        "## Key concepts\n",
        "<hr />\n",
        "\n",
        "![sparkcontext](http://spark.apache.org/docs/latest/img/cluster-overview.png)\n",
        "\n",
        "(Source: [Spark Cluster Mode Overview](https://spark.apache.org/docs/2.4.0/cluster-overview.html).)\n",
        "\n",
        "\n",
        "### Driver\n",
        "\n",
        "-   It creates a `SparkContext`\n",
        "\n",
        "-   Turns the user program into a set of tasks:\n",
        "\n",
        "    -   Logical operations `DAG` -> physical execution plan\n",
        "\n",
        "-   Schedules the tasks to perform on the executors.\n",
        "\n",
        "### SparkSession and SparkContext\n",
        "\n",
        "-    **SparkSession:** entry point for all functionalities of Spark\n",
        "\n",
        "    -  Defines the configuration of the Spark application\n",
        "    -  It is automatically defined as a `spark` variable\n",
        "\n",
        "-   The **SparkContext** performs the connection to the cluster\n",
        "\n",
        "    -   Allows building RDDs from files, lists or other objects\n",
        "    -   Entry point for the low-level API\n",
        "    -   In this Colaboratory notebook (or in the Spark shell) it is automatically defined (`sc` variable)\n",
        "\n",
        "-   Creation on a Python script (see [below](https://colab.research.google.com/drive/1JtPhnvpU1sZnLr2v54EQ-_d1TyeXnTnF#scrollTo=lDeOybgfVT6-)):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvuHVm_ixmdG"
      },
      "source": [
        "\n",
        "### Executors\n",
        "\n",
        "-   Execute each individual task and return the results to the Driver\n",
        "\n",
        "-   Provide a store space in memory for the tasks data\n",
        "\n",
        "\n",
        "### Cluster Manager\n",
        "\n",
        "-   *Pluggable* component on Spark\n",
        "\n",
        "-   YARN, Mesos or Spark Standalone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwyk7y-bDkcn"
      },
      "source": [
        "## Documentation\n",
        "\n",
        "The official documentation for Apache Spark can be found on https://spark.apache.org/docs/latest/\n",
        "\n",
        "The APIs documentation for the different languages is on:\n",
        "\n",
        "  - Python: https://spark.apache.org/docs/latest/api/python/\n",
        "  - Scala: https://spark.apache.org/docs/latest/api/scala/\n",
        "  - Java: https://spark.apache.org/docs/latest/api/java/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.3.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnWUyc46VT-r"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Kjvk_h1AHl"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc0Be08DVT9h"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDeOybgfVT6-"
      },
      "source": [
        "# Another way to create a SparkSession, more detailed\n",
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RCVO3O5VT4p"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXPn9SbVxpwZ"
      },
      "source": [
        "!ls \"$DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}