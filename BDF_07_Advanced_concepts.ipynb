{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_07_Advanced_concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.0\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2023-2024/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 07 - Advanced concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMcqoRTlqa-X"
      },
      "source": [
        "We will show some additional concepts of Apache Spark\n",
        "\n",
        "- How a Spark application is executed\n",
        "- Use of broadcast variables and accummulators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJmCZyf_rGQE"
      },
      "source": [
        "## Execution of a Spark application\n",
        "\n",
        "How the Spark code is executed\n",
        "\n",
        "  - Logic and physical level\n",
        "  - Jobs, stages and tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_l_1lTns58O"
      },
      "source": [
        "### Logic and physical plan\n",
        "\n",
        "From a user code, Spark generates a *logic plan*\n",
        "\n",
        "  -  A DAG with the operations to perform\n",
        "  -  It does not include information on the physical system on which it is going to be executed\n",
        "  -  The *Catalyst* optimiser generates an optimised, logic plan\n",
        "  \n",
        "From the optimised logic plan, a physical plan is created:\n",
        "\n",
        "  - It specifies how the logic plan will be executed in the cluster\n",
        "  - Different execution strategies wil be generated and compared using a cost model\n",
        "      - For example, how to perform a join in function of the characteristics of the data (size, partitions, etc.)\n",
        "\n",
        "The physical plan is executed in the cluster\n",
        "\n",
        "  - The execution is performed on RDDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utmB_4E9wrif"
      },
      "source": [
        "### Jobs, stages and tasks\n",
        "-   As seen, a Spark program defines a DAG connecting the different RDDs\n",
        "    -   *Transformations* create children RDDs from the parent RDDs\n",
        "\n",
        "-   *Actions* translate this DAG into an execution plan by generating a **Spark job**\n",
        "    -   The driver sends a *job* to compute all the RDDs involved in the action\n",
        "    -   A job comprises one or more *stages*\n",
        "    -   Each stage is associated to one or more RDDs from the DAG\n",
        "    -  Stages represent groups of *tasks* which run in parallel\n",
        "        - The stages are processed in order, launching individual tasks to compute segments of the RDDs\n",
        "        - Each task runs one or more transformations on a partition\n",
        "        - Tasks are executed in the cluster nodes\n",
        "    - A stage ends when a *shuffle* operation is performed\n",
        "        - it implies data movement among the cluster nodes\n",
        "\n",
        "\n",
        "-   Pipelining: several RDDs can be computed in the same stage if they verify that:\n",
        "    -   The RDDs can be obtained from their parents without data movement (e.g. *select*, *filter* or *map*), or if any of the RDDs has been cached on memory or disk\n",
        "    - The output of each operation is sent to the input of the following one without going down to disk\n",
        "\n",
        "- Shuffling persistence\n",
        "  -  Before a shuffling operation, data are written to a local disk\n",
        "  -  That allows re-launching failed tasks without the need to recompute all the previous transformations\n",
        "  -  Not performed is the data to shuffle have already been cached (using `cache` or `persist`)\n",
        "\n",
        "\n",
        "-   The *Spark web interface* shows information about the stages and tasks (more information: `toDebugString()` method in the RDDs)\n",
        "\n",
        "- The DataFrame's `explain` method, or RDD's `toDebugString` method shows the physical plan\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lffRje1M2LII"
      },
      "source": [
        "from pyspark.sql.functions import sum,col\n",
        "\n",
        "# Example to visualize the physical plan\n",
        "df1 = spark.range(2, 10000000, 2)\n",
        "df2 = spark.range(2, 10000000, 4)\n",
        "step1 = df1.repartition(5)\n",
        "step12 = df2.repartition(6)\n",
        "step2 = step1.selectExpr(\"id * 5 as id\")\n",
        "step3 = step2.join(step12, [\"id\"])\n",
        "step4 = step3.select(sum(col(\"id\")))\n",
        "\n",
        "step4.collect()\n",
        "step4.explain()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxaKass63KDM"
      },
      "source": [
        "### Broadcast variables\n",
        "\n",
        "-   By default, all shared variables (not RDDs) are sent to all executors\n",
        "\n",
        "    -   They are forwarded on each operation in which they appear\n",
        "\n",
        "-   Broadcast variables: Send, in an efficient way, read-only variables to the workers\n",
        "\n",
        "    -   They are sent only once\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBYFrR5j0jdf"
      },
      "source": [
        "from operator import add\n",
        "\n",
        "# dicc is a broadcast variable\n",
        "dicc=sc.broadcast({\"a\":\"alpha\",\"b\":\"beta\",\"c\":\"gamma\"})\n",
        "\n",
        "rdd=sc.parallelize([(\"a\", 1),(\"b\", 3),(\"a\", -4),(\"c\", 0)])\n",
        "\n",
        "# python 2\n",
        "#reduced_rdd = rdd.reduceByKey(add).map(lambda (x,y): (dicc.value[x],y))\n",
        "\n",
        "# python 3\n",
        "reduced_rdd = rdd.reduceByKey(add).map(lambda x: (dicc.value[x[0]],x[1]))\n",
        "\n",
        "print(reduced_rdd.collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-sYM8TQ3VSq"
      },
      "source": [
        "### Accumulators\n",
        "\n",
        "Aggregate values from the *worker nodes*, which are then sent to the *driver*\n",
        "\n",
        "-   Useful to count events\n",
        "\n",
        "-   Only the driver can access its value\n",
        "\n",
        "-   Accumulators used on RDDs transformations could be incorrect\n",
        "\n",
        "    -   If the RDD is recalculated, the accumulator can be updated\n",
        "\n",
        "    -   This problem does not happen with actions\n",
        "\n",
        "-   By default, accumulators are integers or floats\n",
        "-  \"Custom accumulators\" can be created using [`AccumulatorParam`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.AccumulatorParam)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6GvCBI3tka"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from random import randint\n",
        "\n",
        "# Create a DataFrame from a list of Row objects\n",
        "# with random integers\n",
        "l = [Row(randint(1,10)) for n in range(10000)]\n",
        "df = spark.createDataFrame(l)\n",
        "df.show()\n",
        "# Define an accumulator\n",
        "neven = sc.accumulator(0)\n",
        "\n",
        "# if the number in a row is even, we increment the accumulator\n",
        "def isEven(row):\n",
        "    global neven\n",
        "    if row[\"_1\"]%2 == 0:\n",
        "        neven += 1\n",
        "\n",
        "# Execute the function once per row\n",
        "df.foreach(isEven)\n",
        "\n",
        "print(\"Number of even values: {0}\".format(neven.value))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}